{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import matplotlib.pyplot as plt\n",
    "import synthetic\n",
    "import pickle\n",
    "\n",
    "with open(\"synthetic.data\", \"rb\") as f:\n",
    "    train_ds = pickle.load(f)\n",
    "\n",
    "num_train = len(train_ds[0])\n",
    "plt.scatter(train_ds[0], train_ds[1], s=15, facecolor=\"k\", edgecolor=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from utils import build_masks\n",
    "from type_alias import Batch\n",
    "\n",
    "with open(\"sgd.params\", \"rb\") as f:\n",
    "    base_params = pickle.load(f)\n",
    "\n",
    "wd_coeff = 1.0e-4\n",
    "model_init_fn = lambda rngs: synthetic.Regressor(32, 2, 2, rngs=rngs)\n",
    "init_loss_fn = lambda params: synthetic.neg_log_prior(params, wd_coeff) * num_train\n",
    "\n",
    "\n",
    "def loss_fn(model: nnx.Module, batch: Batch):\n",
    "    x, y = batch\n",
    "    mu, sigma = model(x)\n",
    "    nll = synthetic.neg_log_likel(y, mu, sigma)\n",
    "    nlp = synthetic.neg_log_prior(nnx.state(model).filter(nnx.Param), wd_coeff)\n",
    "    loss = (nll + nlp) * num_train\n",
    "    return loss, (nll, nlp)\n",
    "\n",
    "\n",
    "abstract_model = nnx.eval_shape(lambda: model_init_fn(nnx.Rngs(0)))\n",
    "graphdef, _ = nnx.split(abstract_model)\n",
    "masks = build_masks(abstract_model, to_freeze=[\"fext\"]) \n",
    "\n",
    "import jax.tree_util as tu\n",
    "base_momentum = jax.tree_map(lambda p: jnp.zeros_like(p), base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import uha\n",
    "import importlib\n",
    "\n",
    "importlib.reload(uha)\n",
    "\n",
    "run_uha = uha.build(\n",
    "    base_params, \n",
    "    base_momentum, \n",
    "    graphdef,\n",
    "    masks,\n",
    "    train_ds,\n",
    "    init_loss_fn,\n",
    "    loss_fn, \n",
    "    damper= 0.95, \n",
    "    leapfrog_updates= 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_particles = 1000\n",
    "batch_size = 10\n",
    "overlap = 5\n",
    "step_size = 1.0e-4\n",
    "resample_thres = 0.5\n",
    "num_cycles = 10\n",
    "\n",
    "rngs = nnx.Rngs(42)\n",
    "particles, log_Z_est, resample_cnt = run_uha(\n",
    "    rngs, \n",
    "    num_particles,\n",
    "    batch_size,\n",
    "    overlap,\n",
    "    num_cycles,\n",
    "    damper=0.9,\n",
    "    leapfrog_updates=10,\n",
    "    step_size=step_size,\n",
    "    resample_thres=resample_thres,\n",
    ")\n",
    "\n",
    "num_batches = num_cycles * ((num_train - batch_size) // (batch_size - overlap) + 1)\n",
    "print(f\"Resampling occured {resample_cnt}/{num_batches-1} times.\")\n",
    "print(log_Z_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "x_test = jnp.linspace(-1.2, 1.2, 1000)\n",
    "\n",
    "log_w = particles.log_gamma_k + particles.log_trans - particles.log_gamma_0\n",
    "@nnx.vmap\n",
    "def predict(params: nnx.State):\n",
    "    model = nnx.merge(graphdef, params)\n",
    "    return model(x_test)\n",
    "\n",
    "mus, sigmas = predict(particles.params)\n",
    "mu = jnp.mean(mus, 0)\n",
    "sigma = jnp.sqrt(jnp.mean(sigmas**2, 0) + jnp.var(mus, 0))\n",
    "\n",
    "nw = jnp.exp(log_w - logsumexp(log_w))\n",
    "mu = jnp.sum(mus * nw[..., None], axis=0)\n",
    "sigma = jnp.sqrt(\n",
    "    jnp.sum(sigmas ** 2 * nw[..., None], 0)\n",
    "    + jnp.sum((mus - mu) ** 2 * nw[..., None], 0)\n",
    ")\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.fill_between(\n",
    "    x_test,\n",
    "    mu - 2 * sigma,\n",
    "    mu + 2 * sigma,\n",
    "    alpha=0.2,\n",
    "    facecolor=\"skyblue\",\n",
    "    edgecolor=None,\n",
    ")\n",
    "plt.fill_between(\n",
    "    x_test, mu - sigma, mu + sigma, alpha=0.4, facecolor=\"skyblue\", edgecolor=None\n",
    ") \n",
    "plt.ylim(-3, 3)\n",
    "plt.plot(x_test, mu, label=\"prediction\")\n",
    "plt.plot(train_ds[0], train_ds[1], \"rx\", label=\"training data\", alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
