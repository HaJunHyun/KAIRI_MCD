{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "# Add the subdirectory path to sys.path\n",
    "repo_path = \"/Users/hajunhyeon/Documents/GitHub/KAIRI_MCD/monte-carlo-diffusion\"\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "path1= \"/Users/hajunhyeon/Documents/GitHub/KAIRI_MCD/monte-carlo-diffusion/src\" \n",
    "sys.path.append(path1)\n",
    "# sys.path.append('C:/Users/user/Documents/GitHub/KAIRI_MCD/monte-carlo-diffusion') \n",
    "# %pip install C:/Users/user/Documents/GitHub/KAIRI_MCD/monte-carlo-diffusion \n",
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: equinox in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (0.11.10)\n",
      "Requirement already satisfied: jax!=0.4.27,>=0.4.13 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from equinox) (0.4.30)\n",
      "Requirement already satisfied: jaxtyping>=0.2.20 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from equinox) (0.2.36)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from equinox) (4.10.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (2.0.2)\n",
      "Requirement already satisfied: opt-einsum in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax!=0.4.27,>=0.4.13->equinox) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax!=0.4.27,>=0.4.13->equinox) (3.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpyro in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (0.16.1)\n",
      "Requirement already satisfied: jax>=0.4.25 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from numpyro) (0.4.30)\n",
      "Requirement already satisfied: jaxlib>=0.4.25 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from numpyro) (0.4.30)\n",
      "Requirement already satisfied: multipledispatch in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from numpyro) (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from numpyro) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from numpyro) (4.67.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.25->numpyro) (0.5.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.25->numpyro) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.25->numpyro) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.25->numpyro) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax>=0.4.25->numpyro) (3.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement equinox.optimisers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for equinox.optimisers\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install equinox  \n",
    "%pip install numpyro \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install jax==0.4.25 jaxlib==0.4.25 \n",
    "# %pip install numpy<2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import optax \n",
    "from jax.random import PRNGKey, split\n",
    "from jaxtyping import Array  # type: ignore \n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "\n",
    "class MCDNetResBlock(eqx.Module):\n",
    "    \"\"\"\n",
    "    Residual MLP block from `Score-Based Diffusion meets Annealed Importance Sampling <https://arxiv.org/abs/2208.07698>`_.\n",
    "    \"\"\"\n",
    "\n",
    "    d_t: int = eqx.static_field()\n",
    "    d_h: int = eqx.static_field()\n",
    "    norm: eqx.nn.LayerNorm\n",
    "    t_emb_proj: eqx.nn.Linear\n",
    "    h_proj: eqx.nn.Linear\n",
    "    act: Callable[[Array], Array]\n",
    "    deproj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, d_h, d_t, act=jax.nn.swish, *, key):\n",
    "        super().__init__()\n",
    "        self.d_t = d_t\n",
    "        self.d_h = d_h\n",
    "        self.act = act\n",
    "\n",
    "        keys = split(key, 3)\n",
    "        self.norm = eqx.nn.LayerNorm(d_h)\n",
    "        self.t_emb_proj = eqx.nn.Linear(d_t, 2 * d_h, key=keys[1])\n",
    "        self.h_proj = eqx.nn.Linear(d_h, 2 * d_h, key=keys[0])\n",
    "        self.deproj = eqx.nn.Linear(2 * d_h, d_h, key=keys[2])\n",
    "\n",
    "    def __call__(self, t_emb, h):\n",
    "        h = self.norm(h)\n",
    "        h = self.act(h)\n",
    "        h = self.h_proj(h)\n",
    "        h += self.t_emb_proj(t_emb)\n",
    "        h = self.act(h)\n",
    "        return self.deproj(h)\n",
    "\n",
    "\n",
    "class MCDNet(eqx.Module):\n",
    "    \"\"\"\n",
    "    Residual MLP network similar to the one used in the experiments in `Score-Based Diffusion meets Annealed Importance Sampling <https://arxiv.org/abs/2208.07698>`_.\n",
    "    Parametrized as :math:`c_{\\\\theta}(t, x) + (1 + s_{\\\\theta}(t, x)) \\\\nabla \\\\log \\\\gamma_t(x)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_dim: int = eqx.static_field()\n",
    "    get_score_gamma_t: Callable[[Array, Array], Array]\n",
    "    d_t: int = eqx.static_field()\n",
    "    d_h: int = eqx.static_field()\n",
    "    depth: int = eqx.static_field()\n",
    "    act: Callable[[Array], Array]\n",
    "    # Layers\n",
    "    t_emb: eqx.nn.Linear\n",
    "    x_emb: eqx.nn.Linear\n",
    "    res_layers: List[MCDNetResBlock]\n",
    "    final_layer: eqx.nn.Linear\n",
    "    const_scale: Array\n",
    "    \"\"\"constant multiplying the :math:`c_{\\\\theta}(t, x)` network, initialized to zero.\n",
    "    \"\"\"\n",
    "    score_scale: Array\n",
    "    \"\"\"constant multiplying the :math:`s_{\\\\theta}(t, x)` network, initialized to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_dim: int,\n",
    "        get_score_gamma_t: Callable[[Array, Array], Array],\n",
    "        d_t: int = 4,\n",
    "        d_h: int = 64,\n",
    "        depth: int = 3,\n",
    "        act: Callable[[Array], Array] = jax.nn.swish,\n",
    "        *,\n",
    "        key: PRNGKey\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the network with the output set to :math:`\\\\gamma_t(x)`.\n",
    "\n",
    "        Args:\n",
    "            x_dim: dimensionality of `x`.\n",
    "            get_score_gamma_t: :math:`\\\\gamma_t(x)`, which is used to initialize\n",
    "                the network's output to the standard AIS backward kernel.\n",
    "            d_t: dimensionality of `t` embedding.\n",
    "            d_h: dimensionalixy of `x` embedding.\n",
    "            depth: number of residual MLP blocks.\n",
    "            act: activation function.\n",
    "            key: PRNG key used to initialize layers.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.get_score_gamma_t = get_score_gamma_t\n",
    "        self.d_t = d_t\n",
    "        self.d_h = d_h\n",
    "        self.depth = depth\n",
    "        self.act = act\n",
    "\n",
    "        # Embedding layers\n",
    "        key_t, key_x, key_final, *keys_res = split(key, 3 + depth)\n",
    "        self.t_emb = eqx.nn.Linear(1, d_t, key=key_t)\n",
    "        self.x_emb = eqx.nn.Linear(x_dim, d_h, key=key_x)\n",
    "        self.res_layers = [MCDNetResBlock(d_h, d_t, act, key=k) for k in keys_res]\n",
    "        # Final layer\n",
    "        self.final_layer = eqx.nn.Linear(d_h, 2 * x_dim, key=key_final)\n",
    "        # Scaling for score term\n",
    "        self.const_scale = jnp.array(0.0)\n",
    "        self.score_scale = jnp.array(0.0)\n",
    "\n",
    "    def __call__(self, t: Array, x: Array) -> Array:\n",
    "        t_emb = self.t_emb(t[None])\n",
    "        h = self.x_emb(x)\n",
    "        for res_layer in self.res_layers:\n",
    "            h = res_layer(t_emb, h)\n",
    "        h = self.act(h)\n",
    "        h = self.final_layer(h)\n",
    "        # Scale terms by constants\n",
    "        const = self.const_scale * h[: self.x_dim]\n",
    "        score_factor = 1 + self.score_scale * h[: self.x_dim]\n",
    "        # Combine with score\n",
    "        score_t = self.get_score_gamma_t(t, x)\n",
    "        return const + score_factor * score_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajunhyeon/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx \n",
    "import optax\n",
    "from jax.random import PRNGKey, split\n",
    "from typing import Callable, List\n",
    "from mcd import UnadjustedLangevin\n",
    "import numpyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateNormalDiag:\n",
    "    def __init__(self, loc, scale_diag):\n",
    "        self.loc = loc\n",
    "        self.scale_diag = scale_diag\n",
    "\n",
    "    def sample(self, key):\n",
    "        \"\"\"\n",
    "        Generate a sample from the multivariate normal distribution.\n",
    "        \"\"\"\n",
    "        return self.loc + self.scale_diag * jax.random.normal(key, self.loc.shape)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Compute the log probability of x under the multivariate normal distribution.\n",
    "        \"\"\"\n",
    "        diff = (x - self.loc) / self.scale_diag\n",
    "        log_det = jnp.sum(jnp.log(self.scale_diag))\n",
    "        norm = -0.5 * self.loc.shape[0] * jnp.log(2 * jnp.pi)\n",
    "        return norm - log_det - 0.5 * jnp.sum(diff**2) \n",
    "    \n",
    "class GaussianMixture:\n",
    "    def __init__(self, dim, n_components, stdev, key):\n",
    "        self.dim = dim\n",
    "        self.n_components = n_components\n",
    "        self.stdev = stdev\n",
    "        self.means = dist.Normal(jnp.zeros((dim,)), 3.0).sample(key, (n_components,))\n",
    "    \n",
    "    def _get_component_log_prob(self, x, mean):\n",
    "        return dist.Normal(mean, self.stdev).log_prob(x)\n",
    "    \n",
    "    def get_log_prob(self, x):\n",
    "        \"\"\"Log probability of the Gaussian Mixture.\"\"\"\n",
    "        log_probs = jax.vmap(lambda mean: self._get_component_log_prob(x, mean))(self.means)\n",
    "        return jnp.log(jnp.exp(log_probs).sum() / self.n_components)\n",
    "\n",
    "def get_log_pi(t, x, get_log_gamma, pi_0, T):\n",
    "    \"\"\"Interpolated log probability for diffusion process.\"\"\"\n",
    "    return (1 - t / T) * pi_0.log_prob(x) + (t / T) * get_log_gamma(x)\n",
    "\n",
    "def get_score_pi(t, x, get_log_gamma, pi_0, T):\n",
    "    \"\"\"Gradient of interpolated log probability.\"\"\"\n",
    "    return jax.grad(lambda x: get_log_pi(t, x, get_log_gamma, pi_0, T))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Residual MLP (MCDNet)\n",
    "class ResBlock(eqx.Module):\n",
    "    layer_norm: eqx.nn.LayerNorm\n",
    "    t_linear: eqx.nn.Linear\n",
    "    h_linear: eqx.nn.Linear\n",
    "    final_linear: eqx.nn.Linear\n",
    "    activation: Callable = eqx.static_field()\n",
    "\n",
    "    def __init__(self, t_dim, h_dim, activation=jax.nn.swish, out_dim=None, *, key):\n",
    "        key_t, key_h, key_final = split(key, 3)\n",
    "        self.layer_norm = eqx.nn.LayerNorm(h_dim)\n",
    "        self.t_linear = eqx.nn.Linear(t_dim, 2 * h_dim, key=key_t)\n",
    "        self.h_linear = eqx.nn.Linear(h_dim, 2 * h_dim, key=key_h)\n",
    "        if out_dim is None:\n",
    "            out_dim = h_dim\n",
    "        self.final_linear = eqx.nn.Linear(2 * h_dim, out_dim, key=key_final)\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, t, h):\n",
    "        t_emb = self.t_linear(t)\n",
    "        h = self.layer_norm(h)\n",
    "        h = self.activation(h)\n",
    "        h = self.h_linear(h)\n",
    "        h = t_emb + h\n",
    "        h = self.activation(h)\n",
    "        h = self.final_linear(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Model(eqx.Module):\n",
    "    t_scale: float = eqx.static_field()\n",
    "    t_linear: eqx.nn.Linear\n",
    "    x_linear: eqx.nn.Linear\n",
    "    res_blocks: List[ResBlock]\n",
    "    final_scale: jnp.ndarray \n",
    "    pi_0: MultivariateNormalDiag= eqx.static_field() \n",
    "    T: float = eqx.static_field() \n",
    "    get_log_gamma: Callable= eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self, x_dim, t_dim, h_dim, depth, t_scale, pi_0, T, get_log_gamma, activation=jax.nn.swish, *, key\n",
    "    ):\n",
    "        self.t_scale = t_scale \n",
    "        self.pi_0= pi_0 \n",
    "        self.T= T \n",
    "        self.get_log_gamma= get_log_gamma \n",
    "\n",
    "        keys = split(key, 4)\n",
    "        self.t_linear = eqx.nn.Linear(1, t_dim, key=keys[0])\n",
    "        self.x_linear = eqx.nn.Linear(x_dim, h_dim, key=keys[1]) \n",
    "\n",
    "        res_keys = split(keys[2], depth)\n",
    "        self.res_blocks = []\n",
    "        for i in range(depth - 1):\n",
    "            self.res_blocks.append(ResBlock(t_dim, h_dim, activation=activation, key=res_keys[i]))\n",
    "        self.res_blocks.append(ResBlock(t_dim, h_dim, activation=activation, out_dim=x_dim, key=res_keys[-1]))\n",
    "        \n",
    "        self.final_scale = jnp.zeros(1)\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        t_emb = self.t_linear(jnp.atleast_1d(t) * self.t_scale)\n",
    "        h = self.x_linear(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            h = res_block(t_emb, h)\n",
    "        return self.final_scale * h + get_score_pi(t, x, self.get_log_gamma, self.pi_0, self.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/kcw0q0811sv1mcwy1bs2b5z80000gn/T/ipykernel_23949/3393701735.py:29: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  model = Model(\n",
      "Gaussian Mixture (Steps=64):  50%|████▉     | 99/200 [00:41<00:41,  2.46it/s, loss=2.88e+5]"
     ]
    }
   ],
   "source": [
    "def run_experiment(d, n_timesteps, description):\n",
    "    key = PRNGKey(87)\n",
    "    T = n_timesteps\n",
    "    \n",
    "    # Initialize Gaussian Mixture\n",
    "    n_components = 8\n",
    "    stdev = 1.0\n",
    "    key, subkey = split(key)\n",
    "    gaussian_mixture = GaussianMixture(d, n_components, stdev, subkey)\n",
    "    \n",
    "    # Initialize prior distribution\n",
    "    scale = 3.0\n",
    "    pi_0 = MultivariateNormalDiag(loc=jnp.zeros(d), scale_diag=scale * jnp.ones(d))\n",
    "    \n",
    "    # Initialize ULA\n",
    "    ula = UnadjustedLangevin(\n",
    "        pi_0, \n",
    "        get_log_gamma= gaussian_mixture.get_log_prob, \n",
    "        get_score_gamma_t= lambda t, x: get_score_pi(t, x, gaussian_mixture.get_log_prob, pi_0, T),\n",
    "        n_timesteps= n_timesteps,\n",
    "        T=T\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    key, subkey = split(key)\n",
    "    t_dim = 4\n",
    "    h_dim = 128\n",
    "    depth = 3\n",
    "    model = Model(\n",
    "        x_dim=d, \n",
    "        t_dim= t_dim, \n",
    "        h_dim= h_dim, \n",
    "        depth=depth, \n",
    "        t_scale= ula.delta, \n",
    "        pi_0=pi_0, \n",
    "        T=T, \n",
    "        get_log_gamma= gaussian_mixture.get_log_prob, \n",
    "        key=subkey\n",
    "    )\n",
    "\n",
    "    # Training setup\n",
    "    lr = 5e-3\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    batch_size = 512\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def train_step(model, key, opt_state, batch_size):\n",
    "        keys = split(key, batch_size)\n",
    "        get_loss = lambda model, keys: jax.vmap(ula.get_loss, (None, 0))(model, keys).sum()\n",
    "        loss, grads = eqx.filter_value_and_grad(get_loss)(model, keys)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    # Training loop\n",
    "    n_steps = 200\n",
    "    losses = []\n",
    "    with trange(n_steps, desc=description) as pbar:\n",
    "        for _ in pbar:\n",
    "            key, subkey = split(key)\n",
    "            loss, model, opt_state = train_step(model, subkey, opt_state, batch_size)\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix(loss=losses[-1])\n",
    "\n",
    "    # Evaluate\n",
    "    key, subkey = split(key)\n",
    "    n_samples = 40_000\n",
    "    xs_ais, log_ws_ais = jax.vmap(ula.get_trajectory_ais)(split(subkey, n_samples))\n",
    "    xs_mcd, log_ws_mcd = jax.vmap(ula.get_trajectory_mcd, (None, 0))(model, split(subkey, n_samples))\n",
    "\n",
    "    def compute_log_Z(log_weights):\n",
    "        return jnp.log(jnp.mean(jnp.exp(log_weights)))\n",
    "\n",
    "    log_Z_ais = compute_log_Z(log_ws_ais)\n",
    "    log_Z_mcd = compute_log_Z(log_ws_mcd)\n",
    "\n",
    "    return log_Z_ais, log_Z_mcd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run experiment with dimension 20\n",
    "    d = 20\n",
    "    log_Z_ais, log_Z_mcd = run_experiment(d, 64, \"Gaussian Mixture (Steps=64)\")\n",
    "    print(f\"Gaussian Mixture (Steps=64): AIS Log Z = {log_Z_ais}, MCD Log Z = {log_Z_mcd}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
