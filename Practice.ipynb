{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip \n",
    "%pip install --upgrade jax \n",
    "%pip install \"flax[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Caution(0121): JAX worked in python version 3.9 -> Try new setting in desktop\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "x_jnp= jnp.arange(10) \n",
    "x_np= np.arange(10) \n",
    "\n",
    "print(x_jnp) \n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32 ms ± 55.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "383 μs ± 449 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05): \n",
    "    return lambda_*jnp.where(x>0, x, alpha*jnp.exp(x)-alpha) \n",
    "\n",
    "x= jnp.arange(1000000) \n",
    "%timeit selu(x).block_until_ready() \n",
    "\n",
    "selu_jit= jax.jit(selu) \n",
    "\n",
    "selu_jit(x).block_until_ready() \n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "10.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "f= lambda x: x**3+2*x**2-2*x+1 \n",
    "\n",
    "dfdx= jax.grad(f) \n",
    "d2fdx= jax.grad(dfdx) \n",
    "d3fdx= jax.grad(d2fdx) \n",
    "\n",
    "print(dfdx(1.)) \n",
    "print(d2fdx(1.)) \n",
    "print(d3fdx(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Condition Settings \n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import jax.random as random \n",
    "from jax.scipy.special import logsumexp \n",
    "from jax.scipy.stats import norm , t \n",
    "\n",
    "# Unnomralized target gamma(x), intitial state proposal pi_0(x), number steps K, stepsize eta, annealing schedule (beta_K), damping coefficient h, mass matrix M, score model\n",
    "\n",
    "# Algorithm1 (AIS) \n",
    "\n",
    "def unadjusted_langevin_ais(log_target_fn, log_initial_fn, K, step_size, beta_schedule, rng_key): \n",
    "    rng_key, subkey= random.split(rng_key) \n",
    "    x= random.normal(subkey, shape=(log_initial_fn.ndim,)) \n",
    "    log_w= -log_initial_fn(x)  \n",
    "    x_samples= [x]\n",
    "\n",
    "\n",
    "    for k in range(1, K+1): \n",
    "        rng_key, subkey= random.split(rng_key) \n",
    "        beta_k= beta_schedule[k] \n",
    "        beta_k_minus_1= beta_schedule[k-1]   \n",
    "\n",
    "        def log_intermediate_fn(z): \n",
    "            return beta_k*log_target_fn(z)+ (1-beta_k)*log_initial_fn(z) \n",
    "        def log_previous_fn(z): \n",
    "            return (beta_k_minus_1*log_target_fn(z)+(1-beta_k_minus_1)*log_initial_fn(z)) \n",
    "        \n",
    "        grad_x= jax.grad(log_intermediate_fn)(x) \n",
    "        noise= random.normal(subkey, shape= x.shape) \n",
    "\n",
    "        x_new= x+ step_size*grad_x+jnp.sqrt(2.0*step_size)*noise \n",
    "\n",
    "        grad_x_new= jax.grad(log_intermediate_fn)(x_new)\n",
    "\n",
    "        def log_normal_density(z, mean, var): \n",
    "            return -0.5*jnp.log(2.0*jnp.pi*var)-0.5*jnp.sum((z-mean)**2)/var \n",
    "        \n",
    "        log_F=log_normal_density(\n",
    "            x_new, \n",
    "            mean= x+step_size*grad_x, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_B= log_normal_density( \n",
    "            x, \n",
    "            mean= x_new+step_size*grad_x_new, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_w= log_w+(log_B-log_F) \n",
    "\n",
    "        x=x_new\n",
    "        x_samples.append(x) \n",
    "    \n",
    "    log_w= log_w+log_target_fn(x) \n",
    "\n",
    "    return x_samples, log_w\n",
    "\n",
    "def make_gaussian_mixture_logpdf(d, num_components=8, var=1.0): \n",
    "    rng= jax.random.PRNGKey(12345) \n",
    "    means = random.normal(rng, shape=(num_components, d)) + 3.0\n",
    "\n",
    "    def logpdf(x): \n",
    "        x_reshaped= x[None, :] \n",
    "        diffs= x_reshaped - means \n",
    "\n",
    "        sq_maha= jnp.sum(diffs**2, axis=1) \n",
    "        log_probs= -0.5*d*jnp.log(2*jnp.pi*var)-0.5*sq_maha/var \n",
    "        return logsumexp(log_probs)- jnp.log(num_components) \n",
    "    \n",
    "    logpdf.ndim=d\n",
    "    return logpdf \n",
    "\n",
    "def make_student_t_logpdf(d, df=3): \n",
    "    def logpdf(x): \n",
    "        return jnp.sum(t.logpdf(x, df=df)) \n",
    "    logpdf.ndim=d \n",
    "    return logpdf \n",
    "\n",
    "def make_std_normal_logpdf(d, var=9.0): \n",
    "    def logpdf(x): \n",
    "        return -0.5*d*jnp.log(2*jnp.pi*var)-0.5*jnp.sum(x**2)/var\n",
    "    logpdf.ndim=d \n",
    "    return logpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_on_target(make_log_target_fn, \n",
    "                             make_log_initial_fn, \n",
    "                             K_values=[64, 256], \n",
    "                             dims=[20, 200, 500], \n",
    "                             step_size=0.01, \n",
    "                             n_seeds=3): \n",
    "    results={} \n",
    "    for d in dims:  \n",
    "        log_target_fn= make_log_target_fn(d)  \n",
    "        log_initial_fn= make_log_initial_fn(d) \n",
    "\n",
    "        for K in K_values: \n",
    "            logZ_seeds= [] \n",
    "            for seed in range(n_seeds): \n",
    "                rng= random.PRNGKey(seed) \n",
    "                betas= jnp.linspace(0., 1., K+1) \n",
    "\n",
    "                _, log_w= unadjusted_langevin_ais(\n",
    "                    log_target_fn, \n",
    "                    log_initial_fn, \n",
    "                    K=K, \n",
    "                    step_size= step_size, \n",
    "                    beta_schedule= betas, \n",
    "                    rng_key= rng\n",
    "                )\n",
    "                logZ_seeds.append(np.array(log_w)) \n",
    "                \n",
    "            logZ_seeds= np.array(logZ_seeds) \n",
    "            mean_logZ= logZ_seeds.mean() \n",
    "            sem_logZ= logZ_seeds.std()/np.sqrt(n_seeds) \n",
    "            results[(d, K)]= (mean_logZ, sem_logZ) \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gaussian mixture, d=20, K=64]  logZ = -68.893 ± 11.313\n",
      "[Gaussian mixture, d=20, K=256]  logZ = -49.879 ± 9.933\n",
      "[Gaussian mixture, d=200, K=64]  logZ = -700.062 ± 11.594\n",
      "[Gaussian mixture, d=200, K=256]  logZ = -455.788 ± 6.110\n",
      "[Gaussian mixture, d=500, K=64]  logZ = -1808.832 ± 35.289\n",
      "[Gaussian mixture, d=500, K=256]  logZ = -1176.230 ± 15.941\n",
      "[Student-t df=3, d=20, K=64]  logZ = -0.819 ± 0.593\n",
      "[Student-t df=3, d=20, K=256]  logZ = -0.779 ± 0.422\n",
      "[Student-t df=3, d=200, K=64]  logZ = -9.295 ± 2.183\n",
      "[Student-t df=3, d=200, K=256]  logZ = -8.750 ± 0.819\n",
      "[Student-t df=3, d=500, K=64]  logZ = -27.825 ± 0.926\n",
      "[Student-t df=3, d=500, K=256]  logZ = -23.324 ± 0.796\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    gm_results= run_experiment_on_target(\n",
    "        make_log_target_fn  = lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        K_values=[64, 256], \n",
    "        dims= [20, 200, 500], \n",
    "        step_size=0.01, \n",
    "        n_seeds=3\n",
    "    ) \n",
    "    for (d, K), (mean_logZ, sem_logZ) in gm_results.items(): \n",
    "        print(f\"[Gaussian mixture, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "\n",
    "    t_results= run_experiment_on_target(\n",
    "        make_log_target_fn= lambda dd: make_student_t_logpdf(dd, df=3),  \n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=1.0),\n",
    "        K_values=[64, 256], \n",
    "        dims= [20, 200, 500], \n",
    "        step_size=0.01, \n",
    "        n_seeds=3\n",
    "    ) \n",
    "    for (d, K), (mean_logZ, sem_logZ) in t_results.items(): \n",
    "        print(f\"[Student-t df=3, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 step-size optimization 포함해서 다시 시도 (결과가 쓰레기..) : GPU 로 다시 추후 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dimension d=20 ===\n",
      ">>> K=64, training per-step-size ...\n",
      "[TRAIN] iter=0, neg logZ=1.931, step sizes[0..3]=[0.0101005 0.0101005 0.0099005]\n",
      "[TRAIN] iter=50, neg logZ=1.516, step sizes[0..3]=[0.00968404 0.01000272 0.00891441]\n",
      "[TRAIN] iter=100, neg logZ=1.681, step sizes[0..3]=[0.00907268 0.01075002 0.00889927]\n",
      "[TRAIN] iter=150, neg logZ=1.142, step sizes[0..3]=[0.00939275 0.01049997 0.00921382]\n",
      "[TRAIN] iter=200, neg logZ=0.143, step sizes[0..3]=[0.00918134 0.00974978 0.00926989]\n",
      "[TRAIN] iter=250, neg logZ=2.283, step sizes[0..3]=[0.00935059 0.00929267 0.00961471]\n",
      "Trained step sizes (first few): [0.00987686 0.00864683 0.0100778 ] ...\n",
      "[RESULT] d=20, K=64, mean logZ=-1.374 ± 0.334\n",
      ">>> K=256, training per-step-size ...\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.scipy.stats import t\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "##############################################################################\n",
    "# Target & Initial distributions\n",
    "##############################################################################\n",
    "\n",
    "def make_student_t_logpdf(d, df=3):\n",
    "    \"\"\"\n",
    "    i.i.d. Student-t(df) in R^d\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        return jnp.sum(t.logpdf(x, df=df))\n",
    "    return logpdf\n",
    "\n",
    "def make_std_normal_logpdf(d):\n",
    "    \"\"\"\n",
    "    Standard normal N(0,I_d) in dimension d\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        sq = jnp.sum(x**2)\n",
    "        return -0.5*d*jnp.log(2.*jnp.pi) - 0.5*sq\n",
    "    return logpdf\n",
    "\n",
    "##############################################################################\n",
    "# Unadjusted Langevin AIS w/ per-step-size parameters\n",
    "##############################################################################\n",
    "\n",
    "def unadjusted_langevin_ais_per_step(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    d,\n",
    "    betas,            # shape (K+1,)\n",
    "    step_size_params, # shape (K,)\n",
    "    rng_key\n",
    "):\n",
    "    K = len(betas) - 1\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    # x0 ~ pi0\n",
    "    x = random.normal(subkey, shape=(d,))\n",
    "    # log_w = - log pi0(x0)\n",
    "    log_w = -log_initial_fn(x)\n",
    "\n",
    "    def log_normal_density(z, mean, var):\n",
    "        # isotropic N(mean, var I)\n",
    "        diff = z - mean\n",
    "        sq   = jnp.sum(diff**2)\n",
    "        d_   = diff.shape[0]\n",
    "        return -0.5 * d_ * jnp.log(2.*jnp.pi*var) - 0.5 * (sq/var)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        beta_k = betas[k]\n",
    "\n",
    "        # define gamma_k\n",
    "        def log_intermediate(z):\n",
    "            return beta_k*log_target_fn(z) + (1.-beta_k)*log_initial_fn(z)\n",
    "\n",
    "        eta     = jnp.exp(step_size_params[k-1])\n",
    "        grad_x  = jax.grad(log_intermediate)(x)\n",
    "        noise   = random.normal(subkey, shape=x.shape)\n",
    "        x_new   = x + eta*grad_x + jnp.sqrt(2.*eta)*noise\n",
    "\n",
    "        # weight update\n",
    "        log_F = log_normal_density(\n",
    "            x_new,\n",
    "            mean=x + eta*grad_x,\n",
    "            var=2.*eta\n",
    "        )\n",
    "        grad_x_new = jax.grad(log_intermediate)(x_new)\n",
    "        log_B = log_normal_density(\n",
    "            x,\n",
    "            mean=x_new + eta*grad_x_new,\n",
    "            var=2.*eta\n",
    "        )\n",
    "        log_w += (log_B - log_F)\n",
    "        x = x_new\n",
    "\n",
    "    log_w += log_target_fn(x)\n",
    "    return x, log_w\n",
    "\n",
    "##############################################################################\n",
    "# 2) negative_logZ for training\n",
    "##############################################################################\n",
    "\n",
    "def negative_logZ(params, d, betas, log_target_fn, log_initial_fn, rng_key):\n",
    "    \"\"\"\n",
    "    Runs AIS with step_size_params=params, returns -logZ\n",
    "    \"\"\"\n",
    "    _, log_w = unadjusted_langevin_ais_per_step(\n",
    "        log_target_fn  = log_target_fn,\n",
    "        log_initial_fn = log_initial_fn,\n",
    "        d             = d,\n",
    "        betas         = betas,\n",
    "        step_size_params=params,\n",
    "        rng_key       = rng_key\n",
    "    )\n",
    "    return -log_w\n",
    "\n",
    "##############################################################################\n",
    "# 3) Training routine\n",
    "##############################################################################\n",
    "\n",
    "def train_step_sizes(\n",
    "    d,\n",
    "    K,\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    n_iters=300,\n",
    "    lr=1e-2,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains per-step-size for dimension d, K steps.  Returns the array of step sizes.\n",
    "    \"\"\"\n",
    "    # Initialize log step sizes\n",
    "    theta_init = jnp.log(0.01)*jnp.ones(K)\n",
    "    optimizer = optax.adam(lr)\n",
    "    opt_state = optimizer.init(theta_init)\n",
    "\n",
    "    betas = jnp.linspace(0., 1., K+1)\n",
    "\n",
    "    def loss_fn(params, rng_key):\n",
    "        return negative_logZ(params, d, betas, log_target_fn, log_initial_fn, rng_key)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(params, opt_state, rng_key):\n",
    "        val, grads = jax.value_and_grad(loss_fn)(params, rng_key)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, val\n",
    "\n",
    "    rng = random.PRNGKey(seed)\n",
    "    params = theta_init\n",
    "    for i in range(n_iters):\n",
    "        rng, subkey = random.split(rng)\n",
    "        params, opt_state, current_loss = step(params, opt_state, subkey)\n",
    "        if i % 50 == 0:\n",
    "            s = jnp.exp(params)\n",
    "            print(f\"[TRAIN] iter={i}, neg logZ={current_loss:.3f}, step sizes[0..3]={s[:3]}\")\n",
    "\n",
    "    return jnp.exp(params)  # the actual step sizes\n",
    "\n",
    "##############################################################################\n",
    "# 4) Final experiment procedure to replicate \"paper\" style results\n",
    "##############################################################################\n",
    "\n",
    "def run_experiment_student_t(\n",
    "    dims = [20, 200, 500],\n",
    "    Ks   = [64, 256],\n",
    "    df   = 3,\n",
    "    n_train_iters=300,\n",
    "    n_eval_seeds=5,  # how many times we evaluate for final?\n",
    "    lr=1e-2\n",
    "):\n",
    "    \"\"\"\n",
    "    For each d in dims and K in Ks:\n",
    "      1) Train the step sizes by maximizing logZ\n",
    "      2) Evaluate final logZ over multiple seeds\n",
    "      3) Print mean +- std / sqrt(n_eval_seeds)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for d in dims:\n",
    "        print(f\"\\n=== Dimension d={d} ===\")\n",
    "        # Build the target & initial for dimension d\n",
    "        log_target_fn  = make_student_t_logpdf(d, df=df)\n",
    "        log_initial_fn = make_std_normal_logpdf(d)\n",
    "\n",
    "        for K in Ks:\n",
    "            print(f\">>> K={K}, training per-step-size ...\")\n",
    "            # 1) Train\n",
    "            best_step_sizes = train_step_sizes(\n",
    "                d=d, K=K,\n",
    "                log_target_fn=log_target_fn,\n",
    "                log_initial_fn=log_initial_fn,\n",
    "                n_iters=n_train_iters,\n",
    "                lr=lr,\n",
    "                seed=0\n",
    "            )\n",
    "            print(f\"Trained step sizes (first few): {best_step_sizes[:3]} ...\")\n",
    "\n",
    "            # 2) Evaluate over multiple seeds\n",
    "            betas = jnp.linspace(0., 1., K+1)\n",
    "            logZ_values = []\n",
    "            for seed_eval in range(n_eval_seeds):\n",
    "                rng = random.PRNGKey(seed_eval+100)  # offset seed\n",
    "                _, logw = unadjusted_langevin_ais_per_step(\n",
    "                    log_target_fn  = log_target_fn,\n",
    "                    log_initial_fn = log_initial_fn,\n",
    "                    d             = d,\n",
    "                    betas         = betas,\n",
    "                    step_size_params=jnp.log(best_step_sizes),  # we store them as log in the function\n",
    "                    rng_key       = rng\n",
    "                )\n",
    "                logZ_values.append(np.array(logw))\n",
    "            logZ_values = np.array(logZ_values)\n",
    "            mean_logZ = logZ_values.mean()\n",
    "            sem_logZ  = logZ_values.std() / np.sqrt(n_eval_seeds)\n",
    "\n",
    "            print(f\"[RESULT] d={d}, K={K}, mean logZ={mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "            results[(d,K)] = (mean_logZ, sem_logZ)\n",
    "\n",
    "    return results\n",
    "\n",
    "##############################################################################\n",
    "# \"MAIN\"\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_results = run_experiment_student_t(\n",
    "        dims=[20, 200],  # you can do [20, 200, 500]\n",
    "        Ks=[64, 256],\n",
    "        df=3,\n",
    "        n_train_iters=300,\n",
    "        n_eval_seeds=3,\n",
    "        lr=1e-2\n",
    "    )\n",
    "    print(\"\\nFinal results dictionary:\")\n",
    "    for (d,K), (m, s) in final_results.items():\n",
    "        print(f\"d={d}, K={K} => logZ = {m:.3f} ± {s:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try UHA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog_step(x, p, step_size, log_target_fn, mass_inv=None):  \n",
    "    if mass_inv is None: \n",
    "        mass_inv= jnp.eye(x.shape[0]) \n",
    "    \n",
    "    def potential_energy(z): \n",
    "        return -log_target_fn(z) \n",
    "    \n",
    "    gradient_x= jax.grad(potential_energy)(x) \n",
    "    p_half= p- 0.5*step_size*gradient_x \n",
    "\n",
    "    x_new= x+step_size*(mass_inv@ p_half) \n",
    "\n",
    "    gradient_x_new= jax.grad(potential_energy)(x_new) \n",
    "    p_new= p_half-0.5*step_size*gradient_x_new \n",
    "\n",
    "    return x_new, p_new \n",
    "\n",
    "\n",
    "def leapfrog_iterations(x, p, step_size, iterations, log_target_fn, mass_inv=None): \n",
    "    def body_fn(_, carry): \n",
    "        x_curr, p_curr= carry \n",
    "        x_next, p_next= leapfrog_step(\n",
    "            x_curr, p_curr, step_size, log_target_fn, mass_inv\n",
    "        ) \n",
    "        return None, (x_next, p_next) \n",
    "    _, (x_final, p_final) = jax.lax.scan(body_fn, None, (x,p), length= iterations) \n",
    "    return x_final, p_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unadjusted_hamilton_ais_per_step(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    d,\n",
    "    betas,            # shape (K+1,)\n",
    "    step_size_params, # shape (K,)\n",
    "    h, \n",
    "    mass_inv=None, \n",
    "    rng_key=None\n",
    "): \n",
    "    if rng_key is None: \n",
    "        rng_key= jax.random.PRNGKey(0) \n",
    "    if mass_inv is None: \n",
    "        mass_inv= jnp.eye(d) \n",
    "    \n",
    "    K = len(betas) - 1\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    # x0 ~ pi0\n",
    "    x= random.normal(subkey, shape=(d,)) \n",
    "    rng_key, subkey= random.split(rng_key) \n",
    "    p= random.normal(subkey, shape=(d,))\n",
    "    # log_w = - log pi0(x0)\n",
    "\n",
    "    def log_gaussian_p(p_val, mean, cov):\n",
    "        diff = p_val - mean\n",
    "        sq   = jnp.sum(diff**2)\n",
    "        d_   = p_val.shape[0]\n",
    "        return -0.5 * d_ * jnp.log(2.*jnp.pi) - 0.5*sq # Needs adjustment if not M=I\n",
    "\n",
    "    log_w= -log_initial_fn(x)-log_gaussian_p(p, jnp.zeors(d), jnp.eye(d)) \n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        beta_k = betas[k]\n",
    "\n",
    "        # define gamma_k\n",
    "        def log_intermediate(z):\n",
    "            return beta_k*log_target_fn(z) + (1.-beta_k)*log_initial_fn(z)\n",
    "\n",
    "        rng_key, subkey= random.split(rng_key) \n",
    "        noise= random.normal(subkey, shape=(d, )) \n",
    "\n",
    "        p_mean= h*p \n",
    "        p_std= jnp.sqrt(1.0-h**2) \n",
    "        p_new= p_mean+p_std*noise # Again, we assume M=I\n",
    "\n",
    "        log_num= log_gaussian_p(p, h*p_new, jnp.eye(d)) \n",
    "        log_den= log_gaussian_p(p_new, h*p, jnp.eye(d)) # Again, we assume M=I\n",
    "        log_w= log_w+ (log_num-log_den) \n",
    "\n",
    "        p= p_new\n",
    "\n",
    "        eps= jnp.exp(step_size_params[k-1]) \n",
    "\n",
    "        n_leapfrog= 5 \n",
    "        x_new, p_new= leapfrog_iterations(\n",
    "            x, p, \n",
    "            stepsize= eps, \n",
    "            iterations= n_leapfrog, \n",
    "            log_target_fn= log_intermediate, \n",
    "            mass_inv= mass_inv\n",
    "        )\n",
    "\n",
    "        x, p= x_new, p_new\n",
    "\n",
    "    log_w = log_w+ log_target_fn(x) + log_gaussian_p(p, jnp.zeros(d), jnp.eye(d)) \n",
    "    return x, p, log_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== UHA on Gaussian Mixture, same conditions ========\n",
      "[GMM UHA, d=20, K=64] logZ = -34.254 ± 8.038\n",
      "[GMM UHA, d=20, K=256] logZ = 1.241 ± 2.851\n",
      "[GMM UHA, d=200, K=64] logZ = -364.356 ± 19.768\n",
      "[GMM UHA, d=200, K=256] logZ = 21.797 ± 4.549\n",
      "[GMM UHA, d=500, K=64] logZ = -900.890 ± 22.825\n",
      "[GMM UHA, d=500, K=256] logZ = 79.052 ± 8.423\n",
      "\n",
      "======== UHA on Student-t(df=3), same conditions ========\n",
      "[T UHA, d=20, K=64]  logZ = 2.043 ± 2.491\n",
      "[T UHA, d=20, K=256]  logZ = 0.180 ± 1.663\n",
      "[T UHA, d=200, K=64]  logZ = -26.983 ± 10.888\n",
      "[T UHA, d=200, K=256]  logZ = -35.848 ± 5.678\n",
      "[T UHA, d=500, K=64]  logZ = -58.248 ± 10.049\n",
      "[T UHA, d=500, K=256]  logZ = -76.936 ± 11.134\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.scipy.stats import norm, t\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOGPDFs: Gaussian mixture, Student-t, and Gaussian initial\n",
    "##############################################################################\n",
    "\n",
    "def make_gaussian_mixture_logpdf(d, num_components=8, var=1.0):\n",
    "    \"\"\"\n",
    "    A d-dimensional mixture of Gaussians: means ~ N(0, 3^2 I).\n",
    "    Each component has covariance var*I.\n",
    "    \"\"\"\n",
    "    rng = jax.random.PRNGKey(1234)\n",
    "    means = random.normal(rng, shape=(num_components, d))+3\n",
    "\n",
    "    def logpdf(x):\n",
    "        # shape(x) = (d,)\n",
    "        diffs = x[None, :] - means  # (num_components, d)\n",
    "        sq_maha = jnp.sum(diffs**2, axis=1)\n",
    "        d_ = x.shape[0]\n",
    "        log_comps = -0.5 * d_ * jnp.log(2.*jnp.pi*var) - 0.5*(sq_maha / var)\n",
    "        return logsumexp(log_comps) - jnp.log(num_components)\n",
    "\n",
    "    return logpdf\n",
    "\n",
    "def make_student_t_logpdf(d, df=3):\n",
    "    \"\"\"\n",
    "    i.i.d. Student-t(df) in R^d.\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        return jnp.sum(t.logpdf(x, df=df))\n",
    "    return logpdf\n",
    "\n",
    "def make_std_normal_logpdf(d, var=1.0):\n",
    "    \"\"\"\n",
    "    logpdf of N(0, var I).\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        sq = jnp.sum(x**2)\n",
    "        d_ = x.shape[0]\n",
    "        return -0.5*d_*jnp.log(2.*jnp.pi*var) - 0.5*(sq/var)\n",
    "    return logpdf\n",
    "\n",
    "##############################################################################\n",
    "# 2) GAUSSIAN MOMENTUM HELPERS: log N(p; mean, I)\n",
    "##############################################################################\n",
    "\n",
    "def log_gaussian_p(p, mean):\n",
    "    diff = p - mean\n",
    "    sq   = jnp.sum(diff**2)\n",
    "    d_   = p.shape[0]\n",
    "    return -0.5*d_*jnp.log(2.*jnp.pi) - 0.5*sq\n",
    "\n",
    "##############################################################################\n",
    "# 3) LEAPFROG integrator (M=I) for potential U(x) = -log_intermediate(x)\n",
    "##############################################################################\n",
    "\n",
    "def leapfrog_step(x, p, step_size, log_intermediate):\n",
    "    \"\"\"\n",
    "    One leapfrog step with M=I.\n",
    "    \"\"\"\n",
    "    def potential_energy(z):\n",
    "        return - log_intermediate(z)\n",
    "\n",
    "    gradient_x = jax.grad(potential_energy)(x)\n",
    "    p_half = p - 0.5 * step_size * gradient_x\n",
    "    x_new  = x + step_size * p_half\n",
    "\n",
    "    gradient_x_new = jax.grad(potential_energy)(x_new)\n",
    "    p_new      = p_half - 0.5 * step_size * gradient_x_new\n",
    "    return x_new, p_new\n",
    "\n",
    "def leapfrog_integration(x, p, step_size, n_leapfrog, log_intermediate):\n",
    "    \"\"\"\n",
    "    Repeats 'n_leapfrog' times the leapfrog step.\n",
    "    \"\"\"\n",
    "    for _ in range(n_leapfrog):\n",
    "        x, p = leapfrog_step(x, p, step_size, log_intermediate)\n",
    "    return x, p\n",
    "\n",
    "##############################################################################\n",
    "# 4) Unadjusted Hamiltonian AIS (UHA) with FIXED step size(s)\n",
    "##############################################################################\n",
    "\n",
    "def unadjusted_hamiltonian_ais_fixed(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    d,\n",
    "    K,\n",
    "    step_size,     # scalar or array shape(K,)\n",
    "    damping_coeff, # h in [0,1)\n",
    "    n_leapfrog=5,\n",
    "    rng_key=jax.random.PRNGKey(0)\n",
    "):\n",
    "    \"\"\"\n",
    "    UHA with partial momentum refresh p_k ~ N(h p_{k-1}, (1-h^2)I).\n",
    "    We only do bridging corrections for the momentum update, skipping the\n",
    "    reverse kernel for the leapfrog step => 'truly unadjusted'.\n",
    "\n",
    "    Returns (x_final, p_final, log_w).\n",
    "    \"\"\"\n",
    "    # unify step_size\n",
    "    if isinstance(step_size, float):\n",
    "        step_size = jnp.array([step_size]*K)\n",
    "\n",
    "    # schedule\n",
    "    betas = jnp.linspace(0.0, 1.0, K+1)\n",
    "\n",
    "    # 1) sample x0, p0\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    x = random.normal(subkey, shape=(d,))\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    p = random.normal(subkey, shape=(d,))\n",
    "\n",
    "    # 2) init AIS weight\n",
    "    log_w = -log_initial_fn(x) - log_gaussian_p(p, jnp.zeros(d))\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        beta_k = betas[k]\n",
    "\n",
    "        # define gamma_k\n",
    "        def log_intermediate(z):\n",
    "            return beta_k * log_target_fn(z) + (1 - beta_k)*log_initial_fn(z)\n",
    "\n",
    "        # (a) partial momentum refresh\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        noise = random.normal(subkey, shape=(d,))\n",
    "        p_mean = damping_coeff * p\n",
    "        p_new  = p_mean + jnp.sqrt(1.-damping_coeff**2)*noise\n",
    "\n",
    "        # bridging correction for momentum update\n",
    "        log_num = log_gaussian_p(p, damping_coeff*p_new)\n",
    "        log_den = log_gaussian_p(p_new, damping_coeff*p)\n",
    "        log_w  += (log_num - log_den)\n",
    "        p = p_new\n",
    "\n",
    "        # (b) leapfrog\n",
    "        eps_k = step_size[k-1]\n",
    "        x_new, p_new = leapfrog_integration(\n",
    "            x, p,\n",
    "            step_size   = eps_k,\n",
    "            n_leapfrog  = n_leapfrog,\n",
    "            log_intermediate=log_intermediate\n",
    "        )\n",
    "        # skip bridging correction => unadjusted\n",
    "\n",
    "        x, p = x_new, p_new\n",
    "\n",
    "    # 3) final correction\n",
    "    log_w += log_target_fn(x) + log_gaussian_p(p, jnp.zeros(d))\n",
    "    return x, p, log_w\n",
    "\n",
    "##############################################################################\n",
    "# 5) A \"run_experiment_uha_on_target\" function, same style as ULA\n",
    "##############################################################################\n",
    "\n",
    "def run_experiment_uha_on_target(\n",
    "    make_log_target_fn,\n",
    "    make_log_initial_fn,\n",
    "    K_values=[64,256],\n",
    "    dims=[20,200,500],\n",
    "    step_size=0.01,\n",
    "    damping=0.9,\n",
    "    n_leapfrog=5,\n",
    "    n_seeds=3\n",
    "):\n",
    "    \"\"\"\n",
    "    For each dimension, each K, run unadjusted_hamiltonian_ais_fixed multiple\n",
    "    times with different seeds and average log_w => (mean_logZ, sem).\n",
    "    Returns a dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for d in dims:\n",
    "        # build target & initial logpdf\n",
    "        log_target_fn  = make_log_target_fn(d)\n",
    "        log_initial_fn = make_log_initial_fn(d)\n",
    "\n",
    "        for K in K_values:\n",
    "            logZ_list = []\n",
    "            for seed in range(n_seeds):\n",
    "                rng = random.PRNGKey(seed)\n",
    "                _, _, lw = unadjusted_hamiltonian_ais_fixed(\n",
    "                    log_target_fn  = log_target_fn,\n",
    "                    log_initial_fn = log_initial_fn,\n",
    "                    d             = d,\n",
    "                    K             = K,\n",
    "                    step_size     = step_size,\n",
    "                    damping_coeff = damping,\n",
    "                    n_leapfrog   = n_leapfrog,\n",
    "                    rng_key      = rng\n",
    "                )\n",
    "                logZ_list.append(np.array(lw))\n",
    "\n",
    "            logZ_list = np.array(logZ_list)\n",
    "            mean_logZ = logZ_list.mean()\n",
    "            sem_logZ  = logZ_list.std() / np.sqrt(n_seeds)\n",
    "            results[(d,K)] = (mean_logZ, sem_logZ)\n",
    "    return results\n",
    "\n",
    "##############################################################################\n",
    "# 6) MAIN: replicate the \"same conditions\" as ULA (dims=[20,200,500], Ks=[64,256])\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: test on Gaussian mixture => pi0 = N(0, 9I)\n",
    "    # EXACTLY like the user might have done for ULA\n",
    "    print(\"======== UHA on Gaussian Mixture, same conditions ========\")\n",
    "    gm_results = run_experiment_uha_on_target(\n",
    "        make_log_target_fn  = lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        K_values    = [64, 256],\n",
    "        dims        = [20, 200, 500],\n",
    "        step_size   = 0.01,   # fix step size\n",
    "        damping     = 0.9,\n",
    "        n_leapfrog  = 5,\n",
    "        n_seeds     = 3\n",
    "    )\n",
    "    for (d,K), (mean_logZ, sem_logZ) in gm_results.items():\n",
    "        print(f\"[GMM UHA, d={d}, K={K}] logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "\n",
    "    # Now test on Student-t => pi0 = N(0,I)\n",
    "    print(\"\\n======== UHA on Student-t(df=3), same conditions ========\")\n",
    "    t_results = run_experiment_uha_on_target(\n",
    "        make_log_target_fn  = lambda dd: make_student_t_logpdf(dd, df=3),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=1.0),\n",
    "        K_values    = [64, 256],\n",
    "        dims        = [20, 200, 500],\n",
    "        step_size   = 0.01,\n",
    "        damping     = 0.9,\n",
    "        n_leapfrog  = 5,\n",
    "        n_seeds     = 3\n",
    "    )\n",
    "    for (d,K), (mean_logZ, sem_logZ) in t_results.items():\n",
    "        print(f\"[T UHA, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Parallel UHA on Gaussian Mixture ========\n",
      "[GMM UHA, d=20, K=64] logZ = -104.276 ± 6.549\n",
      "[GMM UHA, d=20, K=256] logZ = -74.712 ± 4.601\n",
      "[GMM UHA, d=200, K=64] logZ = -1452.594 ± 17.588\n",
      "[GMM UHA, d=200, K=256] logZ = -1443.069 ± 17.152\n",
      "[GMM UHA, d=500, K=64] logZ = -3762.187 ± 25.924\n",
      "[GMM UHA, d=500, K=256] logZ = -3746.924 ± 27.925\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "\n",
    "def leapfrog_step_mass(x, p, step_size, log_intermediate, mass_matrix_inv):\n",
    "    \"\"\"\n",
    "    One leapfrog step with custom mass matrix.\n",
    "    \"\"\"\n",
    "    def potential_energy(z):\n",
    "        return -log_intermediate(z)\n",
    "    \n",
    "    gradient_x = jax.grad(potential_energy)(x)\n",
    "    p_half = p - 0.5 * step_size * gradient_x\n",
    "    x_new = x + step_size * mass_matrix_inv @ p_half\n",
    "    gradient_x_new = jax.grad(potential_energy)(x_new)\n",
    "    p_new = p_half - 0.5 * step_size * gradient_x_new\n",
    "    \n",
    "    return x_new, p_new\n",
    "\n",
    "def leapfrog_integration_mass(x, p, step_size, n_leapfrog, log_intermediate, mass_matrix_inv):\n",
    "    \"\"\"\n",
    "    n_leapfrog steps of leapfrog integration with mass matrix.\n",
    "    \"\"\"\n",
    "    def body_fn(_, state):\n",
    "        x, p = state\n",
    "        return leapfrog_step_mass(x, p, step_size, log_intermediate, mass_matrix_inv)\n",
    "    \n",
    "    return jax.lax.fori_loop(0, n_leapfrog, body_fn, (x, p)) \n",
    "\n",
    "def compute_mass_matrix_from_samples(samples):\n",
    "    \"\"\"\n",
    "    Compute empirical mass matrix from samples.\n",
    "    Uses Welford's online algorithm for numerical stability.\n",
    "    \"\"\"\n",
    "    mean = jnp.mean(samples, axis=0)\n",
    "    centered = samples - mean[None, :]\n",
    "    cov = jnp.mean(centered[:, :, None] * centered[:, None, :], axis=0)\n",
    "    # Add small diagonal term for stability\n",
    "    eps = 1e-5\n",
    "    return cov + eps * jnp.eye(cov.shape[0])\n",
    "\n",
    "def adapt_step_size(accept_stats, target_accept=0.65, adaptation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Adapt step size based on acceptance statistics.\n",
    "    Uses Robbins-Monro algorithm.\n",
    "    \"\"\"\n",
    "    log_step_size = jnp.log(step_size)\n",
    "    new_log_step_size = log_step_size + adaptation_rate * (accept_stats - target_accept)\n",
    "    return jnp.exp(new_log_step_size) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 3, 5, 6))\n",
    "def uha_single_chain(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    rng_key,\n",
    "    d,\n",
    "    step_size,\n",
    "    K,\n",
    "    n_leapfrog,\n",
    "    damping_coeff,\n",
    "    mass_matrix=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single chain of UHA with improved numerical stability.\n",
    "    \"\"\"\n",
    "    if mass_matrix is None:\n",
    "        mass_matrix = jnp.eye(d)\n",
    "    mass_matrix_inv = jnp.linalg.inv(mass_matrix)\n",
    "    \n",
    "    # Schedule\n",
    "    betas = jnp.linspace(0.0, 1.0, K+1)\n",
    "    \n",
    "    # Initial samples with scaled mass matrix\n",
    "    key1, key2 = random.split(rng_key)\n",
    "    x = random.multivariate_normal(key1, jnp.zeros(d), mass_matrix)\n",
    "    p = random.multivariate_normal(key2, jnp.zeros(d), mass_matrix)\n",
    "    \n",
    "    # Initialize log weight with stable computation\n",
    "    log_det_mass = jnp.linalg.slogdet(mass_matrix)[1]  # More stable than log(det())\n",
    "    kinetic_energy = 0.5 * p @ mass_matrix_inv @ p\n",
    "    log_w = -log_initial_fn(x) - kinetic_energy - 0.5 * d * jnp.log(2 * jnp.pi) - 0.5 * log_det_mass\n",
    "\n",
    "    def scan_body(carry, beta_k):\n",
    "        x, p, log_w, key = carry\n",
    "        \n",
    "        def log_intermediate(z):\n",
    "            return jnp.where(\n",
    "                beta_k > 0.999,\n",
    "                log_target_fn(z),  # Avoid numerical issues near beta=1\n",
    "                beta_k * log_target_fn(z) + (1 - beta_k) * log_initial_fn(z)\n",
    "            )\n",
    "        \n",
    "        # Momentum refresh with careful scaling\n",
    "        key, subkey = random.split(key)\n",
    "        noise = random.multivariate_normal(subkey, jnp.zeros(d), mass_matrix)\n",
    "        p_mean = damping_coeff * p\n",
    "        p_new = p_mean + jnp.sqrt(1. - damping_coeff**2) * noise\n",
    "        \n",
    "        # Stable momentum bridging correction\n",
    "        log_num = -0.5 * jnp.sum(jnp.square(mass_matrix_inv @ (p - damping_coeff * p_new)))\n",
    "        log_den = -0.5 * jnp.sum(jnp.square(mass_matrix_inv @ (p_new - damping_coeff * p)))\n",
    "        log_w += (log_num - log_den)\n",
    "        \n",
    "        # Scaled leapfrog step size\n",
    "        effective_step_size = step_size / jnp.sqrt(d)\n",
    "        x_new, p_new = leapfrog_integration_mass(\n",
    "            x, p_new, effective_step_size, n_leapfrog, log_intermediate, mass_matrix_inv\n",
    "        )\n",
    "        \n",
    "        return (x_new, p_new, log_w, key), None\n",
    "\n",
    "    # Run chain\n",
    "    (x, p, log_w, _), _ = jax.lax.scan(\n",
    "        scan_body,\n",
    "        (x, p, log_w, rng_key),\n",
    "        betas[1:]\n",
    "    )\n",
    "    \n",
    "    # Final correction with stable computation\n",
    "    kinetic_energy = 0.5 * p @ mass_matrix_inv @ p\n",
    "    log_w += log_target_fn(x) - kinetic_energy - 0.5 * d * jnp.log(2 * jnp.pi) - 0.5 * log_det_mass\n",
    "    \n",
    "    return x, p, log_w\n",
    "\n",
    "def run_parallel_uha_experiment(\n",
    "    make_log_target_fn,\n",
    "    make_log_initial_fn,\n",
    "    K_values=[64, 256],\n",
    "    dims=[20, 200, 500],\n",
    "    damping=0.9,\n",
    "    n_leapfrog=5,\n",
    "    n_chains=10,\n",
    "    make_mass_matrix_fn=None  # Added parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel UHA experiments with dimension-aware parameters.\n",
    "    \n",
    "    Args:\n",
    "        make_mass_matrix_fn: Optional function to create custom mass matrix.\n",
    "                           If None, uses default stable mass matrix.\n",
    "    \"\"\"\n",
    "    if make_mass_matrix_fn is None:\n",
    "        make_mass_matrix_fn = lambda d: jnp.diag(jnp.linspace(0.1, 2.0, d))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for d in dims:\n",
    "        # Dimension-dependent step size\n",
    "        step_size = 0.1 / jnp.sqrt(d)\n",
    "        \n",
    "        log_target_fn = make_log_target_fn(d)\n",
    "        log_initial_fn = make_log_initial_fn(d)\n",
    "        mass_matrix = make_mass_matrix_fn(d)\n",
    "        \n",
    "        for K in K_values:\n",
    "            parallel_uha = jax.vmap(\n",
    "                lambda key: uha_single_chain(\n",
    "                    log_target_fn, log_initial_fn, key, d,\n",
    "                    step_size, K, n_leapfrog, damping, mass_matrix\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            keys = random.split(random.PRNGKey(0), n_chains)\n",
    "            _, _, log_weights = parallel_uha(keys)\n",
    "            \n",
    "            # Stable mean computation\n",
    "            max_log_w = jnp.max(log_weights)\n",
    "            shifted_weights = jnp.exp(log_weights - max_log_w)\n",
    "            mean_logZ = jnp.log(jnp.mean(shifted_weights)) + max_log_w\n",
    "            sem_logZ = jnp.std(log_weights) / jnp.sqrt(n_chains)\n",
    "            \n",
    "            results[(d, K)] = (float(mean_logZ), float(sem_logZ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with custom mass matrix\n",
    "if __name__ == \"__main__\":\n",
    "    def make_diagonal_mass_matrix(d):\n",
    "        # Example: diagonal mass matrix with increasing values\n",
    "        return jnp.diag(jnp.exp(jnp.linspace(0, 1, d)))\n",
    "    \n",
    "    print(\"======== Parallel UHA on Gaussian Mixture ========\")\n",
    "    gm_results = run_parallel_uha_experiment(\n",
    "        make_log_target_fn=lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn=lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        make_mass_matrix_fn=make_diagonal_mass_matrix,  # Custom mass matrix\n",
    "        K_values=[64, 256],\n",
    "        dims=[20, 200, 500],\n",
    "        damping=0.9,\n",
    "        n_leapfrog=5,\n",
    "        n_chains=10  # Number of parallel chains\n",
    "    )\n",
    "    \n",
    "    for (d, K), (mean_logZ, sem_logZ) in gm_results.items():\n",
    "        print(f\"[GMM UHA, d={d}, K={K}] logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 3, 5, 6))\n",
    "def uha_adaptation_phase(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    rng_key,\n",
    "    d,\n",
    "    init_step_size,\n",
    "    K,\n",
    "    n_leapfrog,\n",
    "    damping_coeff,\n",
    "    n_adaptation_steps=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run adaptation phase to tune step size and mass matrix.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    mass_matrix = jnp.eye(d)\n",
    "    step_size = init_step_size\n",
    "    samples = []\n",
    "    accept_stats = []\n",
    "    \n",
    "    def adaptation_step(state, _):\n",
    "        x, p, key, step_size, mass_matrix = state\n",
    "        \n",
    "        # Generate proposal\n",
    "        key, subkey = random.split(key)\n",
    "        x_prop, p_prop, log_w = uha_single_chain(\n",
    "            log_target_fn, log_initial_fn, subkey, d,\n",
    "            step_size, K, n_leapfrog, damping_coeff, mass_matrix\n",
    "        )\n",
    "        \n",
    "        # Accept/reject step (simplified for adaptation)\n",
    "        key, subkey = random.split(key)\n",
    "        accept_prob = jnp.minimum(1.0, jnp.exp(log_w))\n",
    "        accepted = random.bernoulli(subkey, accept_prob)\n",
    "        \n",
    "        # Update statistics\n",
    "        x_new = jnp.where(accepted, x_prop, x)\n",
    "        samples.append(x_new)\n",
    "        accept_stats.append(accepted)\n",
    "        \n",
    "        # Adapt step size and mass matrix\n",
    "        if len(samples) >= 100:  # Wait for burn-in\n",
    "            step_size = adapt_step_size(jnp.mean(accept_stats[-100:]))\n",
    "            mass_matrix = compute_mass_matrix_from_samples(jnp.stack(samples[-100:]))\n",
    "        \n",
    "        return (x_new, p_prop, key, step_size, mass_matrix), None\n",
    "    \n",
    "    # Run adaptation\n",
    "    init_state = (jnp.zeros(d), jnp.zeros(d), rng_key, init_step_size, mass_matrix)\n",
    "    final_state, _ = jax.lax.scan(adaptation_step, init_state, jnp.arange(n_adaptation_steps))\n",
    "    \n",
    "    return final_state[3], final_state[4]  # Return tuned step_size and mass_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
