{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip \n",
    "%pip install --upgrade jax \n",
    "%pip install \"flax[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Caution(0121): JAX worked in python version 3.9 -> Try new setting in desktop\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "x_jnp= jnp.arange(10) \n",
    "x_np= np.arange(10) \n",
    "\n",
    "print(x_jnp) \n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32 ms ± 55.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "383 μs ± 449 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05): \n",
    "    return lambda_*jnp.where(x>0, x, alpha*jnp.exp(x)-alpha) \n",
    "\n",
    "x= jnp.arange(1000000) \n",
    "%timeit selu(x).block_until_ready() \n",
    "\n",
    "selu_jit= jax.jit(selu) \n",
    "\n",
    "selu_jit(x).block_until_ready() \n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "10.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "f= lambda x: x**3+2*x**2-2*x+1 \n",
    "\n",
    "dfdx= jax.grad(f) \n",
    "d2fdx= jax.grad(dfdx) \n",
    "d3fdx= jax.grad(d2fdx) \n",
    "\n",
    "print(dfdx(1.)) \n",
    "print(d2fdx(1.)) \n",
    "print(d3fdx(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Condition Settings \n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import jax.random as random \n",
    "from jax.scipy.special import logsumexp \n",
    "from jax.scipy.stats import norm , t \n",
    "\n",
    "# Unnomralized target gamma(x), intitial state proposal pi_0(x), number steps K, stepsize eta, annealing schedule (beta_K), damping coefficient h, mass matrix M, score model\n",
    "\n",
    "# Algorithm1 (AIS) \n",
    "\n",
    "def unadjusted_langevin_ais(log_target_fn, log_initial_fn, K, step_size, beta_schedule, rng_key): \n",
    "    rng_key, subkey= random.split(rng_key) \n",
    "    x= random.normal(subkey, shape=(log_initial_fn.ndim,)) \n",
    "    log_w= -log_initial_fn(x)  \n",
    "    x_samples= [x]\n",
    "\n",
    "\n",
    "    for k in range(1, K+1): \n",
    "        rng_key, subkey= random.split(rng_key) \n",
    "        beta_k= beta_schedule[k] \n",
    "        beta_k_minus_1= beta_schedule[k-1]   \n",
    "\n",
    "        def log_intermediate_fn(z): \n",
    "            return beta_k*log_target_fn(z)+ (1-beta_k)*log_initial_fn(z) \n",
    "        def log_previous_fn(z): \n",
    "            return (beta_k_minus_1*log_target_fn(z)+(1-beta_k_minus_1)*log_initial_fn(z)) \n",
    "        \n",
    "        grad_x= jax.grad(log_intermediate_fn(x)) \n",
    "        noise= random.normal(subkey, shape= x.shape) \n",
    "\n",
    "        x_new= x+ step_size*grad_x+jnp.sqrt(2.0*step_size)*noise \n",
    "\n",
    "        grad_x_new= jax.grad(log_intermediate_fn)(x_new)\n",
    "\n",
    "        def log_normal_density(z, mean, var): \n",
    "            return -0.5*jnp.log(2.0*jnp.pi*var)-0.5*jnp.sum((z-mean)**2)/var \n",
    "        \n",
    "        log_F=log_normal_density(\n",
    "            x_new, \n",
    "            mean= x+step_size*grad_x, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_B= log_normal_density( \n",
    "            x, \n",
    "            mean= x_new+step_size*grad_x_new, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_w= log_w+(log_B-log_F) \n",
    "\n",
    "        x=x_new\n",
    "        x_samples.append(x) \n",
    "    \n",
    "    log_w= log_w+log_target_fn(x) \n",
    "\n",
    "    return x_samples, log_w\n",
    "\n",
    "def make_gaussian_mixture_logpdf(d, num_components=8, var=1.0): \n",
    "    rng= jax.random.PRNGKey(12345) \n",
    "    means= random.normal(rng, shape=(num_components, d))*3.0 \n",
    "\n",
    "    def logpdf(x): \n",
    "        x_reshaped= x[None, :] \n",
    "        diffs= x_reshaped - means \n",
    "\n",
    "        sq_maha= jnp.sum(diffs**2, axis=1) \n",
    "        log_probs= -0.5*d*jnp.log(2*jnp.pi*var)-0.5*sq_maha/var \n",
    "        return logsumexp(log_probs)- jnp.log(num_components) \n",
    "    \n",
    "    logpdf.ndim=d\n",
    "    return logpdf \n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
