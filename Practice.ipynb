{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip \n",
    "%pip install --upgrade jax \n",
    "%pip install \"flax[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Caution(0121): JAX worked in python version 3.9 -> Try new setting in desktop\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "x_jnp= jnp.arange(10) \n",
    "x_np= np.arange(10) \n",
    "\n",
    "print(x_jnp) \n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32 ms ± 55.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "383 μs ± 449 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05): \n",
    "    return lambda_*jnp.where(x>0, x, alpha*jnp.exp(x)-alpha) \n",
    "\n",
    "x= jnp.arange(1000000) \n",
    "%timeit selu(x).block_until_ready() \n",
    "\n",
    "selu_jit= jax.jit(selu) \n",
    "\n",
    "selu_jit(x).block_until_ready() \n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "10.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "f= lambda x: x**3+2*x**2-2*x+1 \n",
    "\n",
    "dfdx= jax.grad(f) \n",
    "d2fdx= jax.grad(dfdx) \n",
    "d3fdx= jax.grad(d2fdx) \n",
    "\n",
    "print(dfdx(1.)) \n",
    "print(d2fdx(1.)) \n",
    "print(d3fdx(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Condition Settings \n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import jax.random as random \n",
    "from jax.scipy.special import logsumexp \n",
    "from jax.scipy.stats import norm , t \n",
    "\n",
    "# Unnomralized target gamma(x), intitial state proposal pi_0(x), number steps K, stepsize eta, annealing schedule (beta_K), damping coefficient h, mass matrix M, score model\n",
    "\n",
    "# Algorithm1 (AIS) \n",
    "\n",
    "def unadjusted_langevin_ais(log_target_fn, log_initial_fn, K, step_size, beta_schedule, rng_key): \n",
    "    rng_key, subkey= random.split(rng_key) \n",
    "    x= random.normal(subkey, shape=(log_initial_fn.ndim,)) \n",
    "    log_w= -log_initial_fn(x)  \n",
    "    x_samples= [x]\n",
    "\n",
    "\n",
    "    for k in range(1, K+1): \n",
    "        rng_key, subkey= random.split(rng_key) \n",
    "        beta_k= beta_schedule[k] \n",
    "        beta_k_minus_1= beta_schedule[k-1]   \n",
    "\n",
    "        def log_intermediate_fn(z): \n",
    "            return beta_k*log_target_fn(z)+ (1-beta_k)*log_initial_fn(z) \n",
    "        def log_previous_fn(z): \n",
    "            return (beta_k_minus_1*log_target_fn(z)+(1-beta_k_minus_1)*log_initial_fn(z)) \n",
    "        \n",
    "        grad_x= jax.grad(log_intermediate_fn)(x) \n",
    "        noise= random.normal(subkey, shape= x.shape) \n",
    "\n",
    "        x_new= x+ step_size*grad_x+jnp.sqrt(2.0*step_size)*noise \n",
    "\n",
    "        grad_x_new= jax.grad(log_intermediate_fn)(x_new)\n",
    "\n",
    "        def log_normal_density(z, mean, var): \n",
    "            return -0.5*jnp.log(2.0*jnp.pi*var)-0.5*jnp.sum((z-mean)**2)/var \n",
    "        \n",
    "        log_F=log_normal_density(\n",
    "            x_new, \n",
    "            mean= x+step_size*grad_x, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_B= log_normal_density( \n",
    "            x, \n",
    "            mean= x_new+step_size*grad_x_new, \n",
    "            var= 2.0*step_size\n",
    "        ) \n",
    "\n",
    "        log_w= log_w+(log_B-log_F) \n",
    "\n",
    "        x=x_new\n",
    "        x_samples.append(x) \n",
    "    \n",
    "    log_w= log_w+log_target_fn(x) \n",
    "\n",
    "    return x_samples, log_w\n",
    "\n",
    "def make_gaussian_mixture_logpdf(d, num_components=8, var=1.0): \n",
    "    rng= jax.random.PRNGKey(12345) \n",
    "    means = random.normal(rng, shape=(num_components, d)) + 3.0\n",
    "\n",
    "    def logpdf(x): \n",
    "        x_reshaped= x[None, :] \n",
    "        diffs= x_reshaped - means \n",
    "\n",
    "        sq_maha= jnp.sum(diffs**2, axis=1) \n",
    "        log_probs= -0.5*d*jnp.log(2*jnp.pi*var)-0.5*sq_maha/var \n",
    "        return logsumexp(log_probs)- jnp.log(num_components) \n",
    "    \n",
    "    logpdf.ndim=d\n",
    "    return logpdf \n",
    "\n",
    "def make_student_t_logpdf(d, df=3): \n",
    "    def logpdf(x): \n",
    "        return jnp.sum(t.logpdf(x, df=df)) \n",
    "    logpdf.ndim=d \n",
    "    return logpdf \n",
    "\n",
    "def make_std_normal_logpdf(d, var=9.0): \n",
    "    def logpdf(x): \n",
    "        return -0.5*d*jnp.log(2*jnp.pi*var)-0.5*jnp.sum(x**2)/var\n",
    "    logpdf.ndim=d \n",
    "    return logpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_on_target(make_log_target_fn, \n",
    "                             make_log_initial_fn, \n",
    "                             K_values=[64, 256], \n",
    "                             dims=[20, 200, 500], \n",
    "                             step_size=0.01, \n",
    "                             n_seeds=3): \n",
    "    results={} \n",
    "    for d in dims:  \n",
    "        log_target_fn= make_log_target_fn(d)  \n",
    "        log_initial_fn= make_log_initial_fn(d) \n",
    "\n",
    "        for K in K_values: \n",
    "            logZ_seeds= [] \n",
    "            for seed in range(n_seeds): \n",
    "                rng= random.PRNGKey(seed) \n",
    "                betas= jnp.linspace(0., 1., K+1) \n",
    "\n",
    "                _, log_w= unadjusted_langevin_ais(\n",
    "                    log_target_fn, \n",
    "                    log_initial_fn, \n",
    "                    K=K, \n",
    "                    step_size= step_size, \n",
    "                    beta_schedule= betas, \n",
    "                    rng_key= rng\n",
    "                )\n",
    "                logZ_seeds.append(np.array(log_w)) \n",
    "                \n",
    "            logZ_seeds= np.array(logZ_seeds) \n",
    "            mean_logZ= logZ_seeds.mean() \n",
    "            sem_logZ= logZ_seeds.std()/np.sqrt(n_seeds) \n",
    "            results[(d, K)]= (mean_logZ, sem_logZ) \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gaussian mixture, d=20, K=64]  logZ = -68.893 ± 11.313\n",
      "[Gaussian mixture, d=20, K=256]  logZ = -49.879 ± 9.933\n",
      "[Gaussian mixture, d=200, K=64]  logZ = -700.062 ± 11.594\n",
      "[Gaussian mixture, d=200, K=256]  logZ = -455.788 ± 6.110\n",
      "[Gaussian mixture, d=500, K=64]  logZ = -1808.832 ± 35.289\n",
      "[Gaussian mixture, d=500, K=256]  logZ = -1176.230 ± 15.941\n",
      "[Student-t df=3, d=20, K=64]  logZ = -0.819 ± 0.593\n",
      "[Student-t df=3, d=20, K=256]  logZ = -0.779 ± 0.422\n",
      "[Student-t df=3, d=200, K=64]  logZ = -9.295 ± 2.183\n",
      "[Student-t df=3, d=200, K=256]  logZ = -8.750 ± 0.819\n",
      "[Student-t df=3, d=500, K=64]  logZ = -27.825 ± 0.926\n",
      "[Student-t df=3, d=500, K=256]  logZ = -23.324 ± 0.796\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    gm_results= run_experiment_on_target(\n",
    "        make_log_target_fn  = lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        K_values=[64, 256], \n",
    "        dims= [20, 200, 500], \n",
    "        step_size=0.01, \n",
    "        n_seeds=3\n",
    "    ) \n",
    "    for (d, K), (mean_logZ, sem_logZ) in gm_results.items(): \n",
    "        print(f\"[Gaussian mixture, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "\n",
    "    t_results= run_experiment_on_target(\n",
    "        make_log_target_fn= lambda dd: make_student_t_logpdf(dd, df=3),  \n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=1.0),\n",
    "        K_values=[64, 256], \n",
    "        dims= [20, 200, 500], \n",
    "        step_size=0.01, \n",
    "        n_seeds=3\n",
    "    ) \n",
    "    for (d, K), (mean_logZ, sem_logZ) in t_results.items(): \n",
    "        print(f\"[Student-t df=3, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 step-size optimization 포함해서 다시 시도 (결과가 쓰레기..) : GPU 로 다시 추후 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try UHA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog_step(x, p, step_size, log_target_fn, mass_inv=None):  \n",
    "    if mass_inv is None: \n",
    "        mass_inv= jnp.eye(x.shape[0]) \n",
    "    \n",
    "    def potential_energy(z): \n",
    "        return -log_target_fn(z) \n",
    "    \n",
    "    gradient_x= jax.grad(potential_energy)(x) \n",
    "    p_half= p- 0.5*step_size*gradient_x \n",
    "\n",
    "    x_new= x+step_size*(mass_inv@ p_half) \n",
    "\n",
    "    gradient_x_new= jax.grad(potential_energy)(x_new) \n",
    "    p_new= p_half-0.5*step_size*gradient_x_new \n",
    "\n",
    "    return x_new, p_new \n",
    "\n",
    "\n",
    "def leapfrog_iterations(x, p, step_size, iterations, log_target_fn, mass_inv=None): \n",
    "    def body_fn(_, carry): \n",
    "        x_curr, p_curr= carry \n",
    "        x_next, p_next= leapfrog_step(\n",
    "            x_curr, p_curr, step_size, log_target_fn, mass_inv\n",
    "        ) \n",
    "        return None, (x_next, p_next) \n",
    "    _, (x_final, p_final) = jax.lax.scan(body_fn, None, (x,p), length= iterations) \n",
    "    return x_final, p_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unadjusted_hamilton_ais_per_step(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    d,\n",
    "    betas,            # shape (K+1,)\n",
    "    step_size_params, # shape (K,)\n",
    "    h, \n",
    "    mass_inv=None, \n",
    "    rng_key=None\n",
    "): \n",
    "    if rng_key is None: \n",
    "        rng_key= jax.random.PRNGKey(0) \n",
    "    if mass_inv is None: \n",
    "        mass_inv= jnp.eye(d) \n",
    "    \n",
    "    K = len(betas) - 1\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    # x0 ~ pi0\n",
    "    x= random.normal(subkey, shape=(d,)) \n",
    "    rng_key, subkey= random.split(rng_key) \n",
    "    p= random.normal(subkey, shape=(d,))\n",
    "    # log_w = - log pi0(x0)\n",
    "\n",
    "    def log_gaussian_p(p_val, mean, cov):\n",
    "        diff = p_val - mean\n",
    "        sq   = jnp.sum(diff**2)\n",
    "        d_   = p_val.shape[0]\n",
    "        return -0.5 * d_ * jnp.log(2.*jnp.pi) - 0.5*sq # Needs adjustment if not M=I\n",
    "\n",
    "    log_w= -log_initial_fn(x)-log_gaussian_p(p, jnp.zeors(d), jnp.eye(d)) \n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        beta_k = betas[k]\n",
    "\n",
    "        # define gamma_k\n",
    "        def log_intermediate(z):\n",
    "            return beta_k*log_target_fn(z) + (1.-beta_k)*log_initial_fn(z)\n",
    "\n",
    "        rng_key, subkey= random.split(rng_key) \n",
    "        noise= random.normal(subkey, shape=(d, )) \n",
    "\n",
    "        p_mean= h*p \n",
    "        p_std= jnp.sqrt(1.0-h**2) \n",
    "        p_new= p_mean+p_std*noise # Again, we assume M=I\n",
    "\n",
    "        log_num= log_gaussian_p(p, h*p_new, jnp.eye(d)) \n",
    "        log_den= log_gaussian_p(p_new, h*p, jnp.eye(d)) # Again, we assume M=I\n",
    "        log_w= log_w+ (log_num-log_den) \n",
    "\n",
    "        p= p_new\n",
    "\n",
    "        eps= jnp.exp(step_size_params[k-1]) \n",
    "\n",
    "        n_leapfrog= 5 \n",
    "        x_new, p_new= leapfrog_iterations(\n",
    "            x, p, \n",
    "            stepsize= eps, \n",
    "            iterations= n_leapfrog, \n",
    "            log_target_fn= log_intermediate, \n",
    "            mass_inv= mass_inv\n",
    "        )\n",
    "\n",
    "        x, p= x_new, p_new\n",
    "\n",
    "    log_w = log_w+ log_target_fn(x) + log_gaussian_p(p, jnp.zeros(d), jnp.eye(d)) \n",
    "    return x, p, log_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== UHA on Gaussian Mixture, same conditions ========\n",
      "[GMM UHA, d=20, K=64] logZ = -34.254 ± 8.038\n",
      "[GMM UHA, d=20, K=256] logZ = 1.241 ± 2.851\n",
      "[GMM UHA, d=200, K=64] logZ = -364.356 ± 19.768\n",
      "[GMM UHA, d=200, K=256] logZ = 21.797 ± 4.549\n",
      "[GMM UHA, d=500, K=64] logZ = -900.890 ± 22.825\n",
      "[GMM UHA, d=500, K=256] logZ = 79.052 ± 8.423\n",
      "\n",
      "======== UHA on Student-t(df=3), same conditions ========\n",
      "[T UHA, d=20, K=64]  logZ = 2.043 ± 2.491\n",
      "[T UHA, d=20, K=256]  logZ = 0.180 ± 1.663\n",
      "[T UHA, d=200, K=64]  logZ = -26.983 ± 10.888\n",
      "[T UHA, d=200, K=256]  logZ = -35.848 ± 5.678\n",
      "[T UHA, d=500, K=64]  logZ = -58.248 ± 10.049\n",
      "[T UHA, d=500, K=256]  logZ = -76.936 ± 11.134\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.scipy.stats import norm, t\n",
    "\n",
    "##############################################################################\n",
    "# 1) LOGPDFs: Gaussian mixture, Student-t, and Gaussian initial\n",
    "##############################################################################\n",
    "\n",
    "def make_gaussian_mixture_logpdf(d, num_components=8, var=1.0):\n",
    "    \"\"\"\n",
    "    A d-dimensional mixture of Gaussians: means ~ N(0, 3^2 I).\n",
    "    Each component has covariance var*I.\n",
    "    \"\"\"\n",
    "    rng = jax.random.PRNGKey(1234)\n",
    "    means = random.normal(rng, shape=(num_components, d))+3\n",
    "\n",
    "    def logpdf(x):\n",
    "        # shape(x) = (d,)\n",
    "        diffs = x[None, :] - means  # (num_components, d)\n",
    "        sq_maha = jnp.sum(diffs**2, axis=1)\n",
    "        d_ = x.shape[0]\n",
    "        log_comps = -0.5 * d_ * jnp.log(2.*jnp.pi*var) - 0.5*(sq_maha / var)\n",
    "        return logsumexp(log_comps) - jnp.log(num_components)\n",
    "\n",
    "    return logpdf\n",
    "\n",
    "def make_student_t_logpdf(d, df=3):\n",
    "    \"\"\"\n",
    "    i.i.d. Student-t(df) in R^d.\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        return jnp.sum(t.logpdf(x, df=df))\n",
    "    return logpdf\n",
    "\n",
    "def make_std_normal_logpdf(d, var=1.0):\n",
    "    \"\"\"\n",
    "    logpdf of N(0, var I).\n",
    "    \"\"\"\n",
    "    def logpdf(x):\n",
    "        sq = jnp.sum(x**2)\n",
    "        d_ = x.shape[0]\n",
    "        return -0.5*d_*jnp.log(2.*jnp.pi*var) - 0.5*(sq/var)\n",
    "    return logpdf\n",
    "\n",
    "##############################################################################\n",
    "# 2) GAUSSIAN MOMENTUM HELPERS: log N(p; mean, I)\n",
    "##############################################################################\n",
    "\n",
    "def log_gaussian_p(p, mean):\n",
    "    diff = p - mean\n",
    "    sq   = jnp.sum(diff**2)\n",
    "    d_   = p.shape[0]\n",
    "    return -0.5*d_*jnp.log(2.*jnp.pi) - 0.5*sq\n",
    "\n",
    "##############################################################################\n",
    "# 3) LEAPFROG integrator (M=I) for potential U(x) = -log_intermediate(x)\n",
    "##############################################################################\n",
    "\n",
    "def leapfrog_step(x, p, step_size, log_intermediate):\n",
    "    \"\"\"\n",
    "    One leapfrog step with M=I.\n",
    "    \"\"\"\n",
    "    def potential_energy(z):\n",
    "        return - log_intermediate(z)\n",
    "\n",
    "    gradient_x = jax.grad(potential_energy)(x)\n",
    "    p_half = p - 0.5 * step_size * gradient_x\n",
    "    x_new  = x + step_size * p_half\n",
    "\n",
    "    gradient_x_new = jax.grad(potential_energy)(x_new)\n",
    "    p_new      = p_half - 0.5 * step_size * gradient_x_new\n",
    "    return x_new, p_new\n",
    "\n",
    "def leapfrog_integration(x, p, step_size, n_leapfrog, log_intermediate):\n",
    "    \"\"\"\n",
    "    Repeats 'n_leapfrog' times the leapfrog step.\n",
    "    \"\"\"\n",
    "    for _ in range(n_leapfrog):\n",
    "        x, p = leapfrog_step(x, p, step_size, log_intermediate)\n",
    "    return x, p\n",
    "\n",
    "##############################################################################\n",
    "# 4) Unadjusted Hamiltonian AIS (UHA) with FIXED step size(s)\n",
    "##############################################################################\n",
    "\n",
    "def unadjusted_hamiltonian_ais_fixed(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    d,\n",
    "    K,\n",
    "    step_size,     # scalar or array shape(K,)\n",
    "    damping_coeff, # h in [0,1)\n",
    "    n_leapfrog=5,\n",
    "    rng_key=jax.random.PRNGKey(0)\n",
    "):\n",
    "    \"\"\"\n",
    "    UHA with partial momentum refresh p_k ~ N(h p_{k-1}, (1-h^2)I).\n",
    "    We only do bridging corrections for the momentum update, skipping the\n",
    "    reverse kernel for the leapfrog step => 'truly unadjusted'.\n",
    "\n",
    "    Returns (x_final, p_final, log_w).\n",
    "    \"\"\"\n",
    "    # unify step_size\n",
    "    if isinstance(step_size, float):\n",
    "        step_size = jnp.array([step_size]*K)\n",
    "\n",
    "    # schedule\n",
    "    betas = jnp.linspace(0.0, 1.0, K+1)\n",
    "\n",
    "    # 1) sample x0, p0\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    x = random.normal(subkey, shape=(d,))\n",
    "    rng_key, subkey = random.split(rng_key)\n",
    "    p = random.normal(subkey, shape=(d,))\n",
    "\n",
    "    # 2) init AIS weight\n",
    "    log_w = -log_initial_fn(x) - log_gaussian_p(p, jnp.zeros(d))\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        beta_k = betas[k]\n",
    "\n",
    "        # define gamma_k\n",
    "        def log_intermediate(z):\n",
    "            return beta_k * log_target_fn(z) + (1 - beta_k)*log_initial_fn(z)\n",
    "\n",
    "        # (a) partial momentum refresh\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        noise = random.normal(subkey, shape=(d,))\n",
    "        p_mean = damping_coeff * p\n",
    "        p_new  = p_mean + jnp.sqrt(1.-damping_coeff**2)*noise\n",
    "\n",
    "        # bridging correction for momentum update\n",
    "        log_num = log_gaussian_p(p, damping_coeff*p_new)\n",
    "        log_den = log_gaussian_p(p_new, damping_coeff*p)\n",
    "        log_w  += (log_num - log_den)\n",
    "        p = p_new\n",
    "\n",
    "        # (b) leapfrog\n",
    "        eps_k = step_size[k-1]\n",
    "        x_new, p_new = leapfrog_integration(\n",
    "            x, p,\n",
    "            step_size   = eps_k,\n",
    "            n_leapfrog  = n_leapfrog,\n",
    "            log_intermediate=log_intermediate\n",
    "        )\n",
    "        # skip bridging correction => unadjusted\n",
    "\n",
    "        x, p = x_new, p_new\n",
    "\n",
    "    # 3) final correction\n",
    "    log_w += log_target_fn(x) + log_gaussian_p(p, jnp.zeros(d))\n",
    "    return x, p, log_w\n",
    "\n",
    "##############################################################################\n",
    "# 5) A \"run_experiment_uha_on_target\" function, same style as ULA\n",
    "##############################################################################\n",
    "\n",
    "def run_experiment_uha_on_target(\n",
    "    make_log_target_fn,\n",
    "    make_log_initial_fn,\n",
    "    K_values=[64,256],\n",
    "    dims=[20,200,500],\n",
    "    step_size=0.01,\n",
    "    damping=0.9,\n",
    "    n_leapfrog=5,\n",
    "    n_seeds=3\n",
    "):\n",
    "    \"\"\"\n",
    "    For each dimension, each K, run unadjusted_hamiltonian_ais_fixed multiple\n",
    "    times with different seeds and average log_w => (mean_logZ, sem).\n",
    "    Returns a dict.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for d in dims:\n",
    "        # build target & initial logpdf\n",
    "        log_target_fn  = make_log_target_fn(d)\n",
    "        log_initial_fn = make_log_initial_fn(d)\n",
    "\n",
    "        for K in K_values:\n",
    "            logZ_list = []\n",
    "            for seed in range(n_seeds):\n",
    "                rng = random.PRNGKey(seed)\n",
    "                _, _, lw = unadjusted_hamiltonian_ais_fixed(\n",
    "                    log_target_fn  = log_target_fn,\n",
    "                    log_initial_fn = log_initial_fn,\n",
    "                    d             = d,\n",
    "                    K             = K,\n",
    "                    step_size     = step_size,\n",
    "                    damping_coeff = damping,\n",
    "                    n_leapfrog   = n_leapfrog,\n",
    "                    rng_key      = rng\n",
    "                )\n",
    "                logZ_list.append(np.array(lw))\n",
    "\n",
    "            logZ_list = np.array(logZ_list)\n",
    "            mean_logZ = logZ_list.mean()\n",
    "            sem_logZ  = logZ_list.std() / np.sqrt(n_seeds)\n",
    "            results[(d,K)] = (mean_logZ, sem_logZ)\n",
    "    return results\n",
    "\n",
    "##############################################################################\n",
    "# 6) MAIN: replicate the \"same conditions\" as ULA (dims=[20,200,500], Ks=[64,256])\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: test on Gaussian mixture => pi0 = N(0, 9I)\n",
    "    # EXACTLY like the user might have done for ULA\n",
    "    print(\"======== UHA on Gaussian Mixture, same conditions ========\")\n",
    "    gm_results = run_experiment_uha_on_target(\n",
    "        make_log_target_fn  = lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        K_values    = [64, 256],\n",
    "        dims        = [20, 200, 500],\n",
    "        step_size   = 0.01,   # fix step size\n",
    "        damping     = 0.9,\n",
    "        n_leapfrog  = 5,\n",
    "        n_seeds     = 3\n",
    "    )\n",
    "    for (d,K), (mean_logZ, sem_logZ) in gm_results.items():\n",
    "        print(f\"[GMM UHA, d={d}, K={K}] logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n",
    "\n",
    "    # Now test on Student-t => pi0 = N(0,I)\n",
    "    print(\"\\n======== UHA on Student-t(df=3), same conditions ========\")\n",
    "    t_results = run_experiment_uha_on_target(\n",
    "        make_log_target_fn  = lambda dd: make_student_t_logpdf(dd, df=3),\n",
    "        make_log_initial_fn = lambda d: make_std_normal_logpdf(d, var=1.0),\n",
    "        K_values    = [64, 256],\n",
    "        dims        = [20, 200, 500],\n",
    "        step_size   = 0.01,\n",
    "        damping     = 0.9,\n",
    "        n_leapfrog  = 5,\n",
    "        n_seeds     = 3\n",
    "    )\n",
    "    for (d,K), (mean_logZ, sem_logZ) in t_results.items():\n",
    "        print(f\"[T UHA, d={d}, K={K}]  logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Parallel UHA on Gaussian Mixture ========\n",
      "[GMM UHA, d=20, K=64] logZ = -104.276 ± 6.549\n",
      "[GMM UHA, d=20, K=256] logZ = -74.712 ± 4.601\n",
      "[GMM UHA, d=200, K=64] logZ = -1452.594 ± 17.588\n",
      "[GMM UHA, d=200, K=256] logZ = -1443.069 ± 17.152\n",
      "[GMM UHA, d=500, K=64] logZ = -3762.187 ± 25.924\n",
      "[GMM UHA, d=500, K=256] logZ = -3746.924 ± 27.925\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "\n",
    "def leapfrog_step_mass(x, p, step_size, log_intermediate, mass_matrix_inv):\n",
    "    \"\"\"\n",
    "    One leapfrog step with custom mass matrix.\n",
    "    \"\"\"\n",
    "    def potential_energy(z):\n",
    "        return -log_intermediate(z)\n",
    "    \n",
    "    gradient_x = jax.grad(potential_energy)(x)\n",
    "    p_half = p - 0.5 * step_size * gradient_x\n",
    "    x_new = x + step_size * mass_matrix_inv @ p_half\n",
    "    gradient_x_new = jax.grad(potential_energy)(x_new)\n",
    "    p_new = p_half - 0.5 * step_size * gradient_x_new\n",
    "    \n",
    "    return x_new, p_new\n",
    "\n",
    "def leapfrog_integration_mass(x, p, step_size, n_leapfrog, log_intermediate, mass_matrix_inv):\n",
    "    \"\"\"\n",
    "    n_leapfrog steps of leapfrog integration with mass matrix.\n",
    "    \"\"\"\n",
    "    def body_fn(_, state):\n",
    "        x, p = state\n",
    "        return leapfrog_step_mass(x, p, step_size, log_intermediate, mass_matrix_inv)\n",
    "    \n",
    "    return jax.lax.fori_loop(0, n_leapfrog, body_fn, (x, p)) \n",
    "\n",
    "def compute_mass_matrix_from_samples(samples):\n",
    "    \"\"\"\n",
    "    Compute empirical mass matrix from samples.\n",
    "    Uses Welford's online algorithm for numerical stability.\n",
    "    \"\"\"\n",
    "    mean = jnp.mean(samples, axis=0)\n",
    "    centered = samples - mean[None, :]\n",
    "    cov = jnp.mean(centered[:, :, None] * centered[:, None, :], axis=0)\n",
    "    # Add small diagonal term for stability\n",
    "    eps = 1e-5\n",
    "    return cov + eps * jnp.eye(cov.shape[0])\n",
    "\n",
    "def adapt_step_size(accept_stats, target_accept=0.65, adaptation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Adapt step size based on acceptance statistics.\n",
    "    Uses Robbins-Monro algorithm.\n",
    "    \"\"\"\n",
    "    log_step_size = jnp.log(step_size)\n",
    "    new_log_step_size = log_step_size + adaptation_rate * (accept_stats - target_accept)\n",
    "    return jnp.exp(new_log_step_size) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1, 3, 5, 6))\n",
    "def uha_single_chain(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    rng_key,\n",
    "    d,\n",
    "    step_size,\n",
    "    K,\n",
    "    n_leapfrog,\n",
    "    damping_coeff,\n",
    "    mass_matrix=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single chain of UHA with improved numerical stability.\n",
    "    \"\"\"\n",
    "    if mass_matrix is None:\n",
    "        mass_matrix = jnp.eye(d)\n",
    "    mass_matrix_inv = jnp.linalg.inv(mass_matrix)\n",
    "    \n",
    "    # Schedule\n",
    "    betas = jnp.linspace(0.0, 1.0, K+1)\n",
    "    \n",
    "    # Initial samples with scaled mass matrix\n",
    "    key1, key2 = random.split(rng_key)\n",
    "    x = random.multivariate_normal(key1, jnp.zeros(d), mass_matrix)\n",
    "    p = random.multivariate_normal(key2, jnp.zeros(d), mass_matrix)\n",
    "    \n",
    "    # Initialize log weight with stable computation\n",
    "    log_det_mass = jnp.linalg.slogdet(mass_matrix)[1]  # More stable than log(det())\n",
    "    kinetic_energy = 0.5 * p @ mass_matrix_inv @ p\n",
    "    log_w = -log_initial_fn(x) - kinetic_energy - 0.5 * d * jnp.log(2 * jnp.pi) - 0.5 * log_det_mass\n",
    "\n",
    "    def scan_body(carry, beta_k):\n",
    "        x, p, log_w, key = carry\n",
    "        \n",
    "        def log_intermediate(z):\n",
    "            return jnp.where(\n",
    "                beta_k > 0.999,\n",
    "                log_target_fn(z),  # Avoid numerical issues near beta=1\n",
    "                beta_k * log_target_fn(z) + (1 - beta_k) * log_initial_fn(z)\n",
    "            )\n",
    "        \n",
    "        # Momentum refresh with careful scaling\n",
    "        key, subkey = random.split(key)\n",
    "        noise = random.multivariate_normal(subkey, jnp.zeros(d), mass_matrix)\n",
    "        p_mean = damping_coeff * p\n",
    "        p_new = p_mean + jnp.sqrt(1. - damping_coeff**2) * noise\n",
    "        \n",
    "        # Stable momentum bridging correction\n",
    "        log_num = -0.5 * jnp.sum(jnp.square(mass_matrix_inv @ (p - damping_coeff * p_new)))\n",
    "        log_den = -0.5 * jnp.sum(jnp.square(mass_matrix_inv @ (p_new - damping_coeff * p)))\n",
    "        log_w += (log_num - log_den)\n",
    "        \n",
    "        # Scaled leapfrog step size\n",
    "        effective_step_size = step_size / jnp.sqrt(d)\n",
    "        x_new, p_new = leapfrog_integration_mass(\n",
    "            x, p_new, effective_step_size, n_leapfrog, log_intermediate, mass_matrix_inv\n",
    "        )\n",
    "        \n",
    "        return (x_new, p_new, log_w, key), None\n",
    "\n",
    "    # Run chain\n",
    "    (x, p, log_w, _), _ = jax.lax.scan(\n",
    "        scan_body,\n",
    "        (x, p, log_w, rng_key),\n",
    "        betas[1:]\n",
    "    )\n",
    "    \n",
    "    # Final correction with stable computation\n",
    "    kinetic_energy = 0.5 * p @ mass_matrix_inv @ p\n",
    "    log_w += log_target_fn(x) - kinetic_energy - 0.5 * d * jnp.log(2 * jnp.pi) - 0.5 * log_det_mass\n",
    "    \n",
    "    return x, p, log_w\n",
    "\n",
    "def run_parallel_uha_experiment(\n",
    "    make_log_target_fn,\n",
    "    make_log_initial_fn,\n",
    "    K_values=[64, 256],\n",
    "    dims=[20, 200, 500],\n",
    "    damping=0.9,\n",
    "    n_leapfrog=5,\n",
    "    n_chains=10,\n",
    "    make_mass_matrix_fn=None  # Added parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel UHA experiments with dimension-aware parameters.\n",
    "    \n",
    "    Args:\n",
    "        make_mass_matrix_fn: Optional function to create custom mass matrix.\n",
    "                           If None, uses default stable mass matrix.\n",
    "    \"\"\"\n",
    "    if make_mass_matrix_fn is None:\n",
    "        make_mass_matrix_fn = lambda d: jnp.diag(jnp.linspace(0.1, 2.0, d))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for d in dims:\n",
    "        # Dimension-dependent step size\n",
    "        step_size = 0.1 / jnp.sqrt(d)\n",
    "        \n",
    "        log_target_fn = make_log_target_fn(d)\n",
    "        log_initial_fn = make_log_initial_fn(d)\n",
    "        mass_matrix = make_mass_matrix_fn(d)\n",
    "        \n",
    "        for K in K_values:\n",
    "            parallel_uha = jax.vmap(\n",
    "                lambda key: uha_single_chain(\n",
    "                    log_target_fn, log_initial_fn, key, d,\n",
    "                    step_size, K, n_leapfrog, damping, mass_matrix\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            keys = random.split(random.PRNGKey(0), n_chains)\n",
    "            _, _, log_weights = parallel_uha(keys)\n",
    "            \n",
    "            # Stable mean computation\n",
    "            max_log_w = jnp.max(log_weights)\n",
    "            shifted_weights = jnp.exp(log_weights - max_log_w)\n",
    "            mean_logZ = jnp.log(jnp.mean(shifted_weights)) + max_log_w\n",
    "            sem_logZ = jnp.std(log_weights) / jnp.sqrt(n_chains)\n",
    "            \n",
    "            results[(d, K)] = (float(mean_logZ), float(sem_logZ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with custom mass matrix\n",
    "if __name__ == \"__main__\":\n",
    "    def make_diagonal_mass_matrix(d):\n",
    "        # Example: diagonal mass matrix with increasing values\n",
    "        return jnp.diag(jnp.exp(jnp.linspace(0, 1, d)))\n",
    "    \n",
    "    print(\"======== Parallel UHA on Gaussian Mixture ========\")\n",
    "    gm_results = run_parallel_uha_experiment(\n",
    "        make_log_target_fn=lambda d: make_gaussian_mixture_logpdf(d, num_components=8, var=1.0),\n",
    "        make_log_initial_fn=lambda d: make_std_normal_logpdf(d, var=9.0),\n",
    "        make_mass_matrix_fn=make_diagonal_mass_matrix,  # Custom mass matrix\n",
    "        K_values=[64, 256],\n",
    "        dims=[20, 200, 500],\n",
    "        damping=0.9,\n",
    "        n_leapfrog=5,\n",
    "        n_chains=10  # Number of parallel chains\n",
    "    )\n",
    "    \n",
    "    for (d, K), (mean_logZ, sem_logZ) in gm_results.items():\n",
    "        print(f\"[GMM UHA, d={d}, K={K}] logZ = {mean_logZ:.3f} ± {sem_logZ:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 3, 5, 6))\n",
    "def uha_adaptation_phase(\n",
    "    log_target_fn,\n",
    "    log_initial_fn,\n",
    "    rng_key,\n",
    "    d,\n",
    "    init_step_size,\n",
    "    K,\n",
    "    n_leapfrog,\n",
    "    damping_coeff,\n",
    "    n_adaptation_steps=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run adaptation phase to tune step size and mass matrix.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    mass_matrix = jnp.eye(d)\n",
    "    step_size = init_step_size\n",
    "    samples = []\n",
    "    accept_stats = []\n",
    "    \n",
    "    def adaptation_step(state, _):\n",
    "        x, p, key, step_size, mass_matrix = state\n",
    "        \n",
    "        # Generate proposal\n",
    "        key, subkey = random.split(key)\n",
    "        x_prop, p_prop, log_w = uha_single_chain(\n",
    "            log_target_fn, log_initial_fn, subkey, d,\n",
    "            step_size, K, n_leapfrog, damping_coeff, mass_matrix\n",
    "        )\n",
    "        \n",
    "        # Accept/reject step (simplified for adaptation)\n",
    "        key, subkey = random.split(key)\n",
    "        accept_prob = jnp.minimum(1.0, jnp.exp(log_w))\n",
    "        accepted = random.bernoulli(subkey, accept_prob)\n",
    "        \n",
    "        # Update statistics\n",
    "        x_new = jnp.where(accepted, x_prop, x)\n",
    "        samples.append(x_new)\n",
    "        accept_stats.append(accepted)\n",
    "        \n",
    "        # Adapt step size and mass matrix\n",
    "        if len(samples) >= 100:  # Wait for burn-in\n",
    "            step_size = adapt_step_size(jnp.mean(accept_stats[-100:]))\n",
    "            mass_matrix = compute_mass_matrix_from_samples(jnp.stack(samples[-100:]))\n",
    "        \n",
    "        return (x_new, p_prop, key, step_size, mass_matrix), None\n",
    "    \n",
    "    # Run adaptation\n",
    "    init_state = (jnp.zeros(d), jnp.zeros(d), rng_key, init_step_size, mass_matrix)\n",
    "    final_state, _ = jax.lax.scan(adaptation_step, init_state, jnp.arange(n_adaptation_steps))\n",
    "    \n",
    "    return final_state[3], final_state[4]  # Return tuned step_size and mass_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try score model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "import optax\n",
    "\n",
    "##############################################################################\n",
    "# 1) Score Model: Residual MLP\n",
    "##############################################################################\n",
    "\n",
    "class ScoreModel:\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_blocks=3, key=random.PRNGKey(0)):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        key1, key2 = random.split(key)\n",
    "        self.params = {\n",
    "            \"input_proj\": random.normal(key1, shape=(hidden_dim, input_dim)),\n",
    "            \"residuals\": [\n",
    "                {\n",
    "                    \"W1\": random.normal(key2, shape=(hidden_dim, hidden_dim)),\n",
    "                    \"b1\": jnp.zeros(hidden_dim),\n",
    "                    \"W2\": random.normal(key2, shape=(hidden_dim, hidden_dim)),\n",
    "                    \"b2\": jnp.zeros(hidden_dim),\n",
    "                }\n",
    "                for _ in range(num_blocks)\n",
    "            ],\n",
    "            \"output_proj\": random.normal(key2, shape=(input_dim, hidden_dim)),\n",
    "        }\n",
    "\n",
    "    def __call__(self, t, x, params):\n",
    "        \"\"\"Forward pass for the score model.\"\"\"\n",
    "        hidden = jnp.dot(params[\"input_proj\"], x)\n",
    "        for block in params[\"residuals\"]:\n",
    "            residual = jax.nn.swish(jnp.dot(block[\"W1\"], hidden) + block[\"b1\"])\n",
    "            residual = jax.nn.swish(jnp.dot(block[\"W2\"], residual) + block[\"b2\"])\n",
    "            hidden += residual\n",
    "        output = jnp.dot(params[\"output_proj\"], hidden)\n",
    "        return output\n",
    "\n",
    "##############################################################################\n",
    "# 2) Forward Kernel for ULA\n",
    "##############################################################################\n",
    "\n",
    "def forward_kernel(x_k, x_km1, step_size):\n",
    "    \"\"\"\n",
    "    Transition density F_k(x_k | x_k-1) for ULA.\n",
    "    \"\"\"\n",
    "    mean = x_km1 + step_size * grad_target_log_prob(x_km1)\n",
    "    var = 2 * step_size\n",
    "    diff = x_k - mean\n",
    "    d = x_k.shape[0]\n",
    "    log_Fk = -0.5 * d * jnp.log(2 * jnp.pi * var) - 0.5 * jnp.sum(diff**2) / var\n",
    "    return log_Fk\n",
    "\n",
    "def grad_target_log_prob(x):\n",
    "    \"\"\"\n",
    "    Gradient of log target density q(x).\n",
    "    \"\"\"\n",
    "    # Replace this with your target log-prob gradient\n",
    "    return -x  # Example: standard normal target\n",
    "\n",
    "##############################################################################\n",
    "# 3) Loss Function\n",
    "##############################################################################\n",
    "\n",
    "def loss_fn(params, score_model, x_k, x_km1, step_size, t):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss for ULA-MCD.\n",
    "    \"\"\"\n",
    "    # Score model prediction\n",
    "    s_theta = score_model(t, x_k, params)\n",
    "\n",
    "    # Gradient of forward kernel\n",
    "    grad_log_Fk = grad(lambda x: forward_kernel(x, x_km1, step_size))(x_k)\n",
    "\n",
    "    # Loss: MSE between score model and forward kernel gradient\n",
    "    mse_loss = jnp.mean((s_theta - grad_log_Fk) ** 2)\n",
    "    return mse_loss\n",
    "\n",
    "##############################################################################\n",
    "# 4) Training Loop\n",
    "##############################################################################\n",
    "\n",
    "def train_score_model(\n",
    "    score_model, x_data, K, step_size, lr=1e-3, num_steps=1000, batch_size=128\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the score model using ULA-MCD loss.\n",
    "    \"\"\"\n",
    "    optimizer = optax.adam(lr)\n",
    "    params = score_model.params\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(params, opt_state, x_k, x_km1, t, step_size):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(\n",
    "            params, score_model, x_k, x_km1, step_size, t\n",
    "        )\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Sample batch data\n",
    "        idx = np.random.choice(len(x_data), batch_size, replace=False)\n",
    "        x_km1 = x_data[idx]\n",
    "        x_k = x_km1 + jnp.sqrt(2 * step_size) * random.normal(random.PRNGKey(step), shape=x_km1.shape)\n",
    "        t = step / K\n",
    "\n",
    "        # Update model\n",
    "        params, opt_state, loss = train_step(params, opt_state, x_k, x_km1, t, step_size)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss:.6f}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (20,) and (128,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m score_model \u001b[38;5;241m=\u001b[39m ScoreModel(input_dim\u001b[38;5;241m=\u001b[39md)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the score model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m trained_params \u001b[38;5;241m=\u001b[39m train_score_model(\n\u001b[1;32m     17\u001b[0m     score_model, x_data, K, step_size, lr\u001b[38;5;241m=\u001b[39mlr, num_steps\u001b[38;5;241m=\u001b[39mnum_steps, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     18\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 111\u001b[0m, in \u001b[0;36mtrain_score_model\u001b[0;34m(score_model, x_data, K, step_size, lr, num_steps, batch_size)\u001b[0m\n\u001b[1;32m    108\u001b[0m t \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m/\u001b[39m K\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Update model\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m params, opt_state, loss \u001b[38;5;241m=\u001b[39m train_step(params, opt_state, x_k, x_km1, t, step_size)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[26], line 96\u001b[0m, in \u001b[0;36mtrain_score_model.<locals>.train_step\u001b[0;34m(params, opt_state, x_k, x_km1, t, step_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(params, opt_state, x_k, x_km1, t, step_size):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Single training step.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss_fn)(\n\u001b[1;32m     97\u001b[0m         params, score_model, x_k, x_km1, step_size, t\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m     99\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, params)\n\u001b[1;32m    100\u001b[0m     params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[26], line 70\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, score_model, x_k, x_km1, step_size, t)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03mCompute the KL divergence loss for ULA-MCD.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Score model prediction\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m s_theta \u001b[38;5;241m=\u001b[39m score_model(t, x_k, params)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Gradient of forward kernel\u001b[39;00m\n\u001b[1;32m     73\u001b[0m grad_log_Fk \u001b[38;5;241m=\u001b[39m grad(\u001b[38;5;28;01mlambda\u001b[39;00m x: forward_kernel(x, x_km1, step_size))(x_k)\n",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m, in \u001b[0;36mScoreModel.__call__\u001b[0;34m(self, t, x, params)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, x, params):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for the score model.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdot(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m], x)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresiduals\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     33\u001b[0m         residual \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mswish(jnp\u001b[38;5;241m.\u001b[39mdot(block[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m\"\u001b[39m], hidden) \u001b[38;5;241m+\u001b[39m block[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:9061\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   9059\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   9060\u001b[0m     contract_dims \u001b[38;5;241m=\u001b[39m ((a_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (b_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,))\n\u001b[0;32m-> 9061\u001b[0m result \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdot_general(a, b, dimension_numbers\u001b[38;5;241m=\u001b[39m(contract_dims, batch_dims),\n\u001b[1;32m   9062\u001b[0m                          precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[1;32m   9063\u001b[0m                          preferred_element_type\u001b[38;5;241m=\u001b[39mpreferred_element_type)\n\u001b[1;32m   9064\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type,\n\u001b[1;32m   9065\u001b[0m                                           output_weak_type)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/jax/_src/lax/lax.py:3518\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   3516\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3517\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (20,) and (128,)."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    d = 20\n",
    "    K = 64\n",
    "    step_size = 0.01\n",
    "    lr = 1e-3\n",
    "    num_steps = 5000\n",
    "    batch_size = 128\n",
    "\n",
    "    # Sample initial data\n",
    "    x_data = random.normal(random.PRNGKey(0), shape=(1000, d))\n",
    "\n",
    "    # Initialize score model\n",
    "    score_model = ScoreModel(input_dim=d)\n",
    "\n",
    "    # Train the score model\n",
    "    trained_params = train_score_model(\n",
    "        score_model, x_data, K, step_size, lr=lr, num_steps=num_steps, batch_size=batch_size\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
